name: Deploy TFIaC on AWS

"on":
  workflow_dispatch:
    inputs:
      confirmation:
        description: "Type 'deploy' to confirm"
        required: true
        type: string
      environment:
        description: "Environment to deploy (staging/production/experimental)"
        required: true
        type: choice
        options:
        - staging
        - production
        - experimental
        default: "staging"

permissions:
  contents: read
  id-token: write

env:
  CLUSTER_NAME: ${{ vars.CLUSTER_NAME || 'vanillatstodo-cluster' }}
  BUCKET_NAME: ${{ vars.APP_NAME || 'vanillatstodo' }}-terraform-state
  AWS_REGION: ${{ vars.AWS_REGION || 'us-east-2' }}
  TF_VERSION: "1.10.0"
  PROJECT_NAME: ${{ vars.APP_NAME || 'vanillatstodo' }}
  ENVIRONMENT: ${{ github.event.inputs.environment }}
  AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

jobs:
  deploy:
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}

    outputs:
      cluster_status: ${{ env.EKS_STATUS }}
      cluster_endpoint: ${{ steps.eks_output.outputs.cluster_endpoint }}
      cluster_name: ${{ env.CLUSTER_NAME }}

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Debug OIDC token claims
      uses: actions/github-script@v7
      with:
        script: |
          const token = await core.getIDToken('sts.amazonaws.com');
          const payload = JSON.parse(Buffer.from(token.split('.')[1], 'base64').toString());
          core.info(`aud=${payload.aud}`);
          core.info(`sub=${payload.sub}`);
          core.info(`iss=${payload.iss}`);
          core.info(`Full payload: ${JSON.stringify(payload, null, 2)}`);

    - name: Configure AWS credentials (OIDC)
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Ensure S3 State Bucket Exists
      run: |
        chmod +x devops/scripts/infra-manager.sh
        devops/scripts/infra-manager.sh verify

    - name: Set Dynamic Variables
      run: |
        echo "Using GitHub repository variables:"
        echo "CLUSTER_NAME=${{ env.CLUSTER_NAME }}"
        echo "AWS_REGION=${{ env.AWS_REGION }}"
        echo "PROJECT_NAME=${{ env.PROJECT_NAME }}"
        echo "BUCKET_NAME=${{ env.BUCKET_NAME }}"

    - name: Validate Region
      run: |
        # Get current region from AWS CLI configuration or environment
        CURRENT_REGION=${AWS_DEFAULT_REGION:-$(aws configure get region)}
        if [ "$CURRENT_REGION" != "${{ env.AWS_REGION }}" ]; then
          echo "âŒ Wrong region configured: $CURRENT_REGION. Must be ${{ env.AWS_REGION }}"
          exit 1
        fi
        echo "âœ… Region validated: ${{ env.AWS_REGION }}"

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TF_VERSION }}

    - name: Deploy State Resources
      id: state
      run: |
        cd devops/terraform/00_state
        terraform init -reconfigure
        terraform apply -auto-approve \
          -var="aws_region=${{ env.AWS_REGION }}" \
          -var="project_name=${{ env.PROJECT_NAME }}" \
          -var="environment=${{ env.ENVIRONMENT }}"

    - name: Deploy Network Resources
      id: network
      run: |
        cd devops/terraform/01_network
        terraform init -reconfigure \
          -backend-config="bucket=${{ env.BUCKET_NAME }}" \
          -backend-config="key=${{ env.ENVIRONMENT }}/network/terraform.tfstate" \
          -backend-config="region=${{ env.AWS_REGION }}" \
          -backend-config="encrypt=true"
        terraform apply -auto-approve \
          -var="aws_region=${{ env.AWS_REGION }}" \
          -var="project_name=${{ env.PROJECT_NAME }}" \
          -var="environment=${{ env.ENVIRONMENT }}" \
          -var="cluster_name=${{ env.CLUSTER_NAME }}"

    - name: Wait for Network State File in S3
      run: |
        for i in {1..10}; do
          if aws s3api head-object --bucket ${{ env.BUCKET_NAME }} --key "${{ env.ENVIRONMENT }}/network/terraform.tfstate"; then
            echo "State file found!"
            exit 0
          else
            echo "State file not found, retrying in 5 seconds... ($i/10)"
            sleep 5
          fi
        done
        echo "State file not found after waiting."
        exit 1

    - name: Set Environment-Specific Role Name
      run: |
        if [[ "${{ env.ENVIRONMENT }}" == "production" ]]; then
          echo "EKS_CLUSTER_ROLE=production-${{ env.PROJECT_NAME }}-cluster-role" >> $GITHUB_ENV
        else
          echo "EKS_CLUSTER_ROLE=staging-${{ env.PROJECT_NAME }}-cluster-role" >> $GITHUB_ENV
        fi

    - name: Deploy EKS Resources
      id: eks
      run: |
        cd devops/terraform/02_eks
        terraform init -reconfigure \
          -backend-config="bucket=${{ env.BUCKET_NAME }}" \
          -backend-config="key=${{ env.ENVIRONMENT }}/eks/terraform.tfstate" \
          -backend-config="region=${{ env.AWS_REGION }}" \
          -backend-config="encrypt=true"
        terraform apply -auto-approve \
          -var="cluster_name=${{ env.CLUSTER_NAME }}" \
          -var="aws_region=${{ env.AWS_REGION }}" \
          -var="project_name=${{ env.PROJECT_NAME }}" \
          -var="environment=${{ env.ENVIRONMENT }}" \
          -var="cluster_role_name=${{ env.EKS_CLUSTER_ROLE }}"

    - name: Install EBS CSI Driver
      id: ebs_csi
      run: |
        echo "ðŸš€ Installing EBS CSI Driver for persistent volumes..."

        # Install kubectl
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/

        # Install Helm
        curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

        # Update kubeconfig
        aws eks update-kubeconfig \
          --name ${{ env.CLUSTER_NAME }} \
          --region ${{ env.AWS_REGION }}

        # Wait for cluster to be ready
        echo "â³ Waiting for EKS cluster to be ready..."
        aws eks wait cluster-active --name ${{ env.CLUSTER_NAME }}

        # Add OIDC role to EKS aws-auth ConfigMap for Kubernetes API access
        echo "ðŸ”§ Adding OIDC role to EKS aws-auth ConfigMap..."
        ROLE_ARN="${{ secrets.AWS_ROLE_ARN }}"
        
        # Get current aws-auth ConfigMap
        kubectl get configmap aws-auth -n kube-system -o yaml > aws-auth.yaml
        
        # Add the OIDC role to the mapRoles section
        cat >> aws-auth.yaml <<EOF
  - rolearn: $ROLE_ARN
    username: github-actions
    groups:
      - system:masters
EOF
        
        # Apply the updated ConfigMap
        kubectl apply -f aws-auth.yaml

        # Wait for node groups to be ready
        echo "â³ Waiting for node groups to be ready..."
        aws eks wait nodegroup-active --cluster-name ${{ env.CLUSTER_NAME }} --nodegroup-name workers || echo "Node group check completed"

        # Install EBS CSI Driver using Helm
        echo "ðŸ“¦ Adding EKS Helm repository..."
        helm repo add eks https://aws.github.io/eks-charts
        helm repo update

        # Create the ebs-csi-driver service account
        echo "ðŸ”§ Creating EBS CSI Driver service account..."
        kubectl apply -f - --validate=false <<EOF
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: ebs-csi-controller-sa
          namespace: kube-system
          labels:
            app.kubernetes.io/name: aws-ebs-csi-driver
        EOF

        # Create the EBS CSI Driver IAM role
        echo "ðŸ”§ Creating EBS CSI Driver IAM role..."
        EBS_CSI_ROLE_NAME="${{ env.ENVIRONMENT }}-${{ env.PROJECT_NAME }}-ebs-csi-role"

        # Create trust policy for EBS CSI Driver
        cat > ebs-csi-trust-policy.json <<EOF
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Effect": "Allow",
              "Principal": {
                "Federated": "arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:oidc-provider/oidc.eks.${{ env.AWS_REGION }}.amazonaws.com/id/$(aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --query "cluster.identity.oidc.issuer" --output text | cut -d'/' -f5)"
              },
              "Action": "sts:AssumeRoleWithWebIdentity",
              "Condition": {
                "StringEquals": {
                  "oidc.eks.${{ env.AWS_REGION }}.amazonaws.com/id/$(aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --query "cluster.identity.oidc.issuer" --output text | cut -d'/' -f5):sub": "system:serviceaccount:kube-system:ebs-csi-controller-sa",
                  "oidc.eks.${{ env.AWS_REGION }}.amazonaws.com/id/$(aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --query "cluster.identity.oidc.issuer" --output text | cut -d'/' -f5):aud": "sts.amazonaws.com"
                }
              }
            }
          ]
        }
        EOF

        # Create the IAM role
        aws iam create-role \
          --role-name $EBS_CSI_ROLE_NAME \
          --assume-role-policy-document file://ebs-csi-trust-policy.json || echo "Role may already exist"

        # Attach the EBS CSI Driver policy
        aws iam attach-role-policy \
          --role-name $EBS_CSI_ROLE_NAME \
          --policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy || echo "Policy may already be attached"

        # Get the role ARN
        EBS_CSI_ROLE_ARN=$(aws iam get-role --role-name $EBS_CSI_ROLE_NAME --query "Role.Arn" --output text)
        echo "EBS_CSI_ROLE_ARN=$EBS_CSI_ROLE_ARN" >> $GITHUB_ENV

        # Annotate the service account with the role ARN
        kubectl annotate serviceaccount ebs-csi-controller-sa \
          -n kube-system \
          eks.amazonaws.com/role-arn=$EBS_CSI_ROLE_ARN

        # Install EBS CSI Driver using Helm
        echo "ðŸ“¦ Installing EBS CSI Driver..."
        helm upgrade --install aws-ebs-csi-driver eks/aws-ebs-csi-driver \
          --namespace kube-system \
          --set controller.serviceAccount.create=false \
          --set controller.serviceAccount.name=ebs-csi-controller-sa \
          --wait --timeout=300s

        # Verify installation
        echo "ðŸ” Verifying EBS CSI Driver installation..."
        kubectl get pods -n kube-system -l app=ebs-csi-controller
        kubectl get csidriver ebs.csi.aws.com

        echo "âœ… EBS CSI Driver installed successfully!"

    - name: Skip IAM Resources (Using Manual OIDC Role)
      id: iam
      run: |
        echo "â„¹ï¸ Skipping IAM deployment - using manually created OIDC role"
        echo "Role ARN: ${{ secrets.AWS_ROLE_ARN }}"

    - name: Deploy Monitoring Resources
      id: monitoring
      run: |
        cd devops/terraform/03_monitoring
        terraform init -reconfigure \
          -backend-config="bucket=${{ env.BUCKET_NAME }}" \
          -backend-config="key=${{ env.ENVIRONMENT }}/monitoring/terraform.tfstate" \
          -backend-config="region=${{ env.AWS_REGION }}" \
          -backend-config="encrypt=true"
        terraform apply -auto-approve \
          -var="aws_region=${{ env.AWS_REGION }}" \
          -var="project_name=${{ env.PROJECT_NAME }}" \
          -var="environment=${{ env.ENVIRONMENT }}"

    - name: Verify EKS Deployment
      id: verify_eks
      run: |
        echo "ðŸ” Verifying EKS deployment..."

        # Wait for cluster to be active
        echo "â³ Waiting for cluster to be active..."
        aws eks wait cluster-active --name ${{ env.CLUSTER_NAME }}

        # Get cluster endpoint
        CLUSTER_ENDPOINT=$(aws eks describe-cluster \
          --name ${{ env.CLUSTER_NAME }} \
          --query "cluster.endpoint" \
          --output text)

        # Get cluster status
        CLUSTER_STATUS=$(aws eks describe-cluster \
          --name ${{ env.CLUSTER_NAME }} \
          --query "cluster.status" \
          --output text)

        echo "CLUSTER_ENDPOINT=$CLUSTER_ENDPOINT" >> $GITHUB_ENV
        echo "CLUSTER_STATUS=$CLUSTER_STATUS" >> $GITHUB_ENV

        if [ "$CLUSTER_STATUS" = "ACTIVE" ]; then
          echo "âœ… EKS cluster is active"
          echo "CLUSTER_VERIFIED=true" >> $GITHUB_ENV
        else
          echo "âŒ EKS cluster is not active"
          echo "CLUSTER_VERIFIED=false" >> $GITHUB_ENV
          exit 1
        fi

    - name: Verify Network Resources
      id: verify_network
      run: |
        echo "ðŸ” Verifying network resources..."

        # Check VPC
        VPC_ID=$(aws ec2 describe-vpcs \
          --filters "Name=tag:Project,Values=${{ env.PROJECT_NAME }}" \
                   "Name=tag:Environment,Values=${{ env.ENVIRONMENT }}" \
          --query "Vpcs[0].VpcId" \
          --output text)

        if [ "$VPC_ID" != "None" ]; then
          echo "âœ… VPC found: $VPC_ID"
        else
          echo "âŒ VPC not found"
          exit 1
        fi

        # Check subnets
        SUBNET_COUNT=$(aws ec2 describe-subnets \
          --filters "Name=vpc-id,Values=$VPC_ID" \
          --query "length(Subnets)")

        if [ "$SUBNET_COUNT" -ge 4 ]; then
          echo "âœ… Found $SUBNET_COUNT subnets"
        else
          echo "âŒ Expected at least 4 subnets, found $SUBNET_COUNT"
          exit 1
        fi

    - name: Verify Monitoring Resources
      id: verify_monitoring
      run: |
        echo "ðŸ” Verifying monitoring resources..."

        # Check CloudWatch Dashboard with correct naming pattern
        DASHBOARD_NAME="${{ env.PROJECT_NAME }}-${{ env.ENVIRONMENT }}-${{ env.CLUSTER_NAME }}-dashboard"
        echo "ðŸ” Looking for dashboard: $DASHBOARD_NAME"

        DASHBOARD=$(aws cloudwatch get-dashboard \
          --dashboard-name "$DASHBOARD_NAME" \
          --query "DashboardBody" \
          --output text 2>/dev/null || echo "None")

        if [ "$DASHBOARD" != "None" ]; then
          echo "âœ… CloudWatch Dashboard found: $DASHBOARD_NAME"
        else
          echo "âŒ CloudWatch Dashboard not found: $DASHBOARD_NAME"
          
          # List all dashboards for debugging
          echo "ðŸ” Available dashboards:"
          aws cloudwatch list-dashboards --query "DashboardEntries[*].DashboardName" --output table || true
          exit 1
        fi

        # Check CloudWatch Alarms with better error handling
        echo "ðŸ” Checking CloudWatch Alarms..."
        ALARM_COUNT=$(aws cloudwatch describe-alarms \
          --query "length(MetricAlarms[?contains(Tags[?Key=='Project'].Value, '${{ env.PROJECT_NAME }}') && contains(Tags[?Key=='Environment'].Value, '${{ env.ENVIRONMENT }}')])" \
          --output text 2>/dev/null || echo "0")

        if [ "$ALARM_COUNT" -gt 0 ]; then
          echo "âœ… Found $ALARM_COUNT CloudWatch Alarms"
        else
          echo "âš ï¸ No CloudWatch Alarms found with tags"
          
          # Try alternative search by name pattern
          PATTERN_ALARMS=$(aws cloudwatch describe-alarms \
            --query "MetricAlarms[?contains(AlarmName, '${{ env.ENVIRONMENT }}-${{ env.PROJECT_NAME }}-${{ env.CLUSTER_NAME }}')]" \
            --output text 2>/dev/null || echo "")
          
          if [ ! -z "$PATTERN_ALARMS" ] && [ "$PATTERN_ALARMS" != "None" ]; then
            echo "âœ… Found alarms by name pattern"
          else
            echo "âŒ No CloudWatch Alarms found"
            echo "ðŸ” Available alarms:"
            aws cloudwatch describe-alarms --query "MetricAlarms[*].AlarmName" --output table || true
            exit 1
          fi
        fi

    - name: Export EKS Details
      id: eks_output
      if: success()
      run: |
        ENDPOINT=$(aws eks describe-cluster \
          --name ${{ env.CLUSTER_NAME }} \
          --query "cluster.endpoint" \
          --output text)
        echo "cluster_endpoint=${ENDPOINT}" >> $GITHUB_OUTPUT

    - name: Export Manual IAM Role ARN
      id: iam_output
      if: success()
      run: |
        echo "github_actions_role_arn=${{ secrets.AWS_ROLE_ARN }}" >> $GITHUB_OUTPUT
        echo "âœ… Using Manual GitHub Actions Role ARN: ${{ secrets.AWS_ROLE_ARN }}"

    - name: Generate Resource Summary
      id: resource_summary
      if: success()
      run: |
        echo "ðŸ” Generating comprehensive resource summary..."

        # Get VPC ID for filtering
        VPC_ID=$(aws ec2 describe-vpcs \
          --filters "Name=tag:Project,Values=${{ env.PROJECT_NAME }}" \
                   "Name=tag:Environment,Values=${{ env.ENVIRONMENT }}" \
          --query "Vpcs[0].VpcId" \
          --output text)

        if [ "$VPC_ID" = "None" ] || [ -z "$VPC_ID" ]; then
          echo "âŒ VPC not found for resource counting"
          exit 1
        fi

        echo "ðŸ“Š VPC ID: $VPC_ID"

        # Count Subnets
        SUBNET_COUNT=$(aws ec2 describe-subnets \
          --filters "Name=vpc-id,Values=$VPC_ID" \
          --query "length(Subnets)" \
          --output text)

        # Count Route Tables
        RT_COUNT=$(aws ec2 describe-route-tables \
          --filters "Name=vpc-id,Values=$VPC_ID" \
          --query "length(RouteTables)" \
          --output text)

        # Count Internet Gateways
        IGW_COUNT=$(aws ec2 describe-internet-gateways \
          --filters "Name=attachment.vpc-id,Values=$VPC_ID" \
          --query "length(InternetGateways)" \
          --output text)

        # Count VPC Endpoints
        ENDPOINT_COUNT=$(aws ec2 describe-vpc-endpoints \
          --filters "Name=vpc-id,Values=$VPC_ID" \
          --query "length(VpcEndpoints)" \
          --output text)

        # Count Security Groups
        SG_COUNT=$(aws ec2 describe-security-groups \
          --filters "Name=vpc-id,Values=$VPC_ID" \
          --query "length(SecurityGroups)" \
          --output text)

        # Count NAT Gateways
        NAT_COUNT=$(aws ec2 describe-nat-gateways \
          --filter "Name=vpc-id,Values=$VPC_ID" \
          --query "length(NatGateways)" \
          --output text)

        # Count EIPs
        EIP_COUNT=$(aws ec2 describe-addresses \
          --filters "Name=tag:Project,Values=${{ env.PROJECT_NAME }}" \
                   "Name=tag:Environment,Values=${{ env.ENVIRONMENT }}" \
          --query "length(Addresses)" \
          --output text)

        # Count Network ACLs
        NACL_COUNT=$(aws ec2 describe-network-acls \
          --filters "Name=vpc-id,Values=$VPC_ID" \
          --query "length(NetworkAcls)" \
          --output text)

        # Count EKS Clusters
        EKS_COUNT=$(aws eks list-clusters \
          --query "length(clusters[?@ && contains(@, '${{ env.CLUSTER_NAME }}')])" \
          --output text)

        # Count CloudWatch Dashboards
        DASHBOARD_COUNT=$(aws cloudwatch list-dashboards \
          --query "length(DashboardEntries[?DashboardName && contains(DashboardName, '${{ env.PROJECT_NAME }}') && contains(DashboardName, '${{ env.ENVIRONMENT }}')])" \
          --output text)

        # Count CloudWatch Alarms  
        ALARM_COUNT=$(aws cloudwatch describe-alarms \
          --query "length(MetricAlarms[?Tags && length(Tags[?Key=='Project' && Value=='${{ env.PROJECT_NAME }}']) > \`0\` && length(Tags[?Key=='Environment' && Value=='${{ env.ENVIRONMENT }}']) > \`0\`])" \
          --output text)

        # Store counts in environment variables
        echo "SUBNET_COUNT=$SUBNET_COUNT" >> $GITHUB_ENV
        echo "RT_COUNT=$RT_COUNT" >> $GITHUB_ENV
        echo "IGW_COUNT=$IGW_COUNT" >> $GITHUB_ENV
        echo "ENDPOINT_COUNT=$ENDPOINT_COUNT" >> $GITHUB_ENV
        echo "SG_COUNT=$SG_COUNT" >> $GITHUB_ENV
        echo "NAT_COUNT=$NAT_COUNT" >> $GITHUB_ENV
        echo "EIP_COUNT=$EIP_COUNT" >> $GITHUB_ENV
        echo "NACL_COUNT=$NACL_COUNT" >> $GITHUB_ENV
        echo "EKS_COUNT=$EKS_COUNT" >> $GITHUB_ENV
        echo "DASHBOARD_COUNT=$DASHBOARD_COUNT" >> $GITHUB_ENV
        echo "ALARM_COUNT=$ALARM_COUNT" >> $GITHUB_ENV

        # Calculate total resources
        TOTAL_RESOURCES=$((SUBNET_COUNT + RT_COUNT + IGW_COUNT + ENDPOINT_COUNT + SG_COUNT + NAT_COUNT + EIP_COUNT + NACL_COUNT + EKS_COUNT + DASHBOARD_COUNT + ALARM_COUNT))
        echo "TOTAL_RESOURCES=$TOTAL_RESOURCES" >> $GITHUB_ENV

        echo "âœ… Resource counting completed!"

    - name: Display Resource Summary
      if: success()
      run: |
        echo "### ðŸ—ï¸ Infrastructure Deployment Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Environment:** \`${{ env.ENVIRONMENT }}\`" >> $GITHUB_STEP_SUMMARY
        echo "**Project:** \`${{ env.PROJECT_NAME }}\`" >> $GITHUB_STEP_SUMMARY
        echo "**Region:** \`${{ env.AWS_REGION }}\`" >> $GITHUB_STEP_SUMMARY
        echo "**Timestamp:** \`$(date -u '+%Y-%m-%d %H:%M:%S UTC')\`" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        echo "### ðŸ“Š Resource Counts" >> $GITHUB_STEP_SUMMARY
        echo "| Resource Type | Count | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|---------------|-------|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| **VPC** | 1 | âœ… |" >> $GITHUB_STEP_SUMMARY
        echo "| **Subnets** | ${{ env.SUBNET_COUNT }} | âœ… |" >> $GITHUB_STEP_SUMMARY
        echo "| **Route Tables** | ${{ env.RT_COUNT }} | âœ… |" >> $GITHUB_STEP_SUMMARY
        echo "| **Internet Gateways** | ${{ env.IGW_COUNT }} | âœ… |" >> $GITHUB_STEP_SUMMARY
        echo "| **VPC Endpoints** | ${{ env.ENDPOINT_COUNT }} | âœ… |" >> $GITHUB_STEP_SUMMARY
        echo "| **Security Groups** | ${{ env.SG_COUNT }} | âœ… |" >> $GITHUB_STEP_SUMMARY
        echo "| **NAT Gateways** | ${{ env.NAT_COUNT }} | âœ… |" >> $GITHUB_STEP_SUMMARY
        echo "| **Elastic IPs** | ${{ env.EIP_COUNT }} | âœ… |" >> $GITHUB_STEP_SUMMARY
        echo "| **Network ACLs** | ${{ env.NACL_COUNT }} | âœ… |" >> $GITHUB_STEP_SUMMARY
        echo "| **EKS Clusters** | ${{ env.EKS_COUNT }} | âœ… |" >> $GITHUB_STEP_SUMMARY
        echo "| **CloudWatch Dashboards** | ${{ env.DASHBOARD_COUNT }} | âœ… |" >> $GITHUB_STEP_SUMMARY
        echo "| **CloudWatch Alarms** | ${{ env.ALARM_COUNT }} | âœ… |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        echo "### ðŸ”— EKS Cluster Details" >> $GITHUB_STEP_SUMMARY
        echo "| Property | Value |" >> $GITHUB_STEP_SUMMARY
        echo "|----------|-------|" >> $GITHUB_STEP_SUMMARY
        echo "| **Cluster Name** | \`${{ env.CLUSTER_NAME }}\` |" >> $GITHUB_STEP_SUMMARY
        echo "| **Cluster Status** | \`${{ env.CLUSTER_STATUS }}\` |" >> $GITHUB_STEP_SUMMARY
        echo "| **Cluster Endpoint** | \`${{ env.CLUSTER_ENDPOINT }}\` |" >> $GITHUB_STEP_SUMMARY
        echo "| **Kubernetes Version** | \`1.31\` |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        echo "### ðŸŽ¯ Deployment Status" >> $GITHUB_STEP_SUMMARY
        if [ "${{ env.CLUSTER_VERIFIED }}" = "true" ]; then
          echo "**Status:** âœ… **SUCCESS** - All resources deployed successfully!" >> $GITHUB_STEP_SUMMARY
        else
          echo "**Status:** âŒ **FAILED** - Some resources failed to deploy" >> $GITHUB_STEP_SUMMARY
        fi
        echo "" >> $GITHUB_STEP_SUMMARY

        echo "**Configuration:**" >> $GITHUB_STEP_SUMMARY
        echo "- Terraform Version: \`${{ env.TF_VERSION }}\`" >> $GITHUB_STEP_SUMMARY
        echo "- AWS Region: \`${{ env.AWS_REGION }}\`" >> $GITHUB_STEP_SUMMARY
        echo "- Project: \`${{ env.PROJECT_NAME }}\`" >> $GITHUB_STEP_SUMMARY
