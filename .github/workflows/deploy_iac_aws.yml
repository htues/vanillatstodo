name: Deploy TFIaC on AWS

"on":
  workflow_dispatch:
    inputs:
      confirmation:
        description: "Type 'deploy' to confirm"
        required: true
        type: string
      environment:
        description: "Environment to deploy (staging/production/experimental)"
        required: true
        type: choice
        options:
        - staging
        - production
        - experimental
        default: "staging"

permissions:
  contents: read
  id-token: write

env:
  CLUSTER_NAME: ${{ vars.CLUSTER_NAME || 'vanillatstodo-cluster' }}
  BUCKET_NAME: ${{ vars.APP_NAME || 'vanillatstodo' }}-terraform-state
  AWS_REGION: ${{ vars.AWS_REGION || 'us-east-2' }}
  TF_VERSION: "1.10.0"
  PROJECT_NAME: ${{ vars.APP_NAME || 'vanillatstodo' }}
  ENVIRONMENT: ${{ github.event.inputs.environment }}
  AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

jobs:
  deploy:
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}

    outputs:
      cluster_status: ${{ env.EKS_STATUS }}
      cluster_endpoint: ${{ steps.eks_output.outputs.cluster_endpoint }}
      cluster_name: ${{ env.CLUSTER_NAME }}

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Debug OIDC token claims
      uses: actions/github-script@v7
      with:
        script: |
          const token = await core.getIDToken('sts.amazonaws.com');
          const payload = JSON.parse(Buffer.from(token.split('.')[1], 'base64').toString());
          core.info(`aud=${payload.aud}`);
          core.info(`sub=${payload.sub}`);
          core.info(`iss=${payload.iss}`);
          core.info(`Full payload: ${JSON.stringify(payload, null, 2)}`);

    - name: Configure AWS credentials (OIDC)
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Ensure S3 State Bucket Exists
      run: |
        chmod +x devops/scripts/infra-manager.sh
        devops/scripts/infra-manager.sh verify

    - name: Set Dynamic Variables
      run: |
        echo "Using GitHub repository variables:"
        echo "CLUSTER_NAME=${{ env.CLUSTER_NAME }}"
        echo "AWS_REGION=${{ env.AWS_REGION }}"
        echo "PROJECT_NAME=${{ env.PROJECT_NAME }}"
        echo "BUCKET_NAME=${{ env.BUCKET_NAME }}"

    - name: Validate Region
      run: |
        # Get current region from AWS CLI configuration or environment
        CURRENT_REGION=${AWS_DEFAULT_REGION:-$(aws configure get region)}
        if [ "$CURRENT_REGION" != "${{ env.AWS_REGION }}" ]; then
          echo "❌ Wrong region configured: $CURRENT_REGION. Must be ${{ env.AWS_REGION }}"
          exit 1
        fi
        echo "✅ Region validated: ${{ env.AWS_REGION }}"

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TF_VERSION }}

    - name: Deploy State Resources
      id: state
      run: |
        cd devops/terraform/00_state
        terraform init -reconfigure
        terraform apply -auto-approve \
          -var="aws_region=${{ env.AWS_REGION }}" \
          -var="project_name=${{ env.PROJECT_NAME }}" \
          -var="environment=${{ env.ENVIRONMENT }}"

    - name: Deploy Network Resources
      id: network
      run: |
        cd devops/terraform/01_network
        terraform init -reconfigure \
          -backend-config="bucket=${{ env.BUCKET_NAME }}" \
          -backend-config="key=${{ env.ENVIRONMENT }}/network/terraform.tfstate" \
          -backend-config="region=${{ env.AWS_REGION }}" \
          -backend-config="encrypt=true"
        terraform apply -auto-approve \
          -var="aws_region=${{ env.AWS_REGION }}" \
          -var="project_name=${{ env.PROJECT_NAME }}" \
          -var="environment=${{ env.ENVIRONMENT }}" \
          -var="cluster_name=${{ env.CLUSTER_NAME }}"

    - name: Wait for Network State File in S3
      run: |
        for i in {1..10}; do
          if aws s3api head-object --bucket ${{ env.BUCKET_NAME }} --key "${{ env.ENVIRONMENT }}/network/terraform.tfstate"; then
            echo "State file found!"
            exit 0
          else
            echo "State file not found, retrying in 5 seconds... ($i/10)"
            sleep 5
          fi
        done
        echo "State file not found after waiting."
        exit 1

    - name: Set Environment-Specific Role Name
      run: |
        if [[ "${{ env.ENVIRONMENT }}" == "production" ]]; then
          echo "EKS_CLUSTER_ROLE=production-${{ env.PROJECT_NAME }}-cluster-role" >> $GITHUB_ENV
        else
          echo "EKS_CLUSTER_ROLE=staging-${{ env.PROJECT_NAME }}-cluster-role" >> $GITHUB_ENV
        fi

    - name: Deploy EKS Resources
      id: eks
      run: |
        cd devops/terraform/02_eks
        terraform init -reconfigure \
          -backend-config="bucket=${{ env.BUCKET_NAME }}" \
          -backend-config="key=${{ env.ENVIRONMENT }}/eks/terraform.tfstate" \
          -backend-config="region=${{ env.AWS_REGION }}" \
          -backend-config="encrypt=true"
        terraform apply -auto-approve \
          -var="cluster_name=${{ env.CLUSTER_NAME }}" \
          -var="aws_region=${{ env.AWS_REGION }}" \
          -var="project_name=${{ env.PROJECT_NAME }}" \
          -var="environment=${{ env.ENVIRONMENT }}" \
          -var="cluster_role_name=${{ env.EKS_CLUSTER_ROLE }}" \
          -var="gha_actions_role_arn=${{ secrets.AWS_ROLE_ARN }}"

    - name: Install EBS CSI Driver
      id: ebs_csi
      run: |
        echo "🚀 Installing EBS CSI Driver for persistent volumes..."

        # Install kubectl
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/

        # Install Helm
        curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

        # Update kubeconfig and force using the pipeline's IAM role
        echo "🔧 Configuring kubectl for EKS access..."

        # Get the EKS cluster's service role for debugging
        CLUSTER_ROLE_ARN=$(aws eks describe-cluster \
          --name ${{ env.CLUSTER_NAME }} \
          --query "cluster.roleArn" \
          --output text)

        echo "EKS cluster role: $CLUSTER_ROLE_ARN"

        # Update kubeconfig using current credentials (already assumed via OIDC action)
        aws eks update-kubeconfig \
          --name ${{ env.CLUSTER_NAME }} \
          --region ${{ env.AWS_REGION }}

        # Quick diagnostics for auth/context
        echo "🔎 AWS identity:" && aws sts get-caller-identity || true
        echo "🔎 kubectl version (client):" && kubectl version --client=true --output=yaml || true
        echo "🔎 Current kube context:" && kubectl config current-context || true
        echo "🔎 kubeconfig users:" && kubectl config view -o jsonpath='{.users[*].name}' || true
        echo "" # newline
        echo "🔎 Attempting to fetch token..." && aws eks get-token --cluster-name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} >/dev/null && echo "Token OK" || echo "Token fetch failed"

        # Wait for cluster to be ready
        echo "⏳ Waiting for EKS cluster to be ready..."
        aws eks wait cluster-active --name ${{ env.CLUSTER_NAME }}

        # Wait for node groups to be ready
        echo "⏳ Waiting for node groups to be ready..."
        aws eks wait nodegroup-active --cluster-name ${{ env.CLUSTER_NAME }} --nodegroup-name workers || echo "Node group check completed"

        # Install EBS CSI Driver using Helm
        echo "📦 Adding EKS Helm repository..."
        helm repo add eks https://aws.github.io/eks-charts
        helm repo update

        # Wait briefly for EKS access entry/credentials to propagate
        echo "⏳ Waiting for EKS access propagation..."
        for i in {1..12}; do
          if kubectl auth can-i get namespaces >/dev/null 2>&1; then
            echo "✅ EKS access verified."
            break
          else
            echo "...still waiting for RBAC/credential propagation ($i/12)" && sleep 10
          fi
        done
        if ! kubectl auth can-i get namespaces >/dev/null 2>&1; then
          echo "❌ EKS access not yet propagated after waiting. Aborting."
          exit 1
        fi

        # Do not create the service account manually; the EKS add-on will manage it

        # Create the EBS CSI Driver IAM role
        echo "🔧 Creating EBS CSI Driver IAM role..."
        EBS_CSI_ROLE_NAME="${{ env.ENVIRONMENT }}-${{ env.PROJECT_NAME }}-ebs-csi-role"

        # Create trust policy for EBS CSI Driver using jq to avoid heredoc indentation issues
        ISSUER_ID=$(aws eks describe-cluster \
          --name ${{ env.CLUSTER_NAME }} \
          --query "cluster.identity.oidc.issuer" \
          --output text | cut -d'/' -f5)
        jq -n \
          --arg acct "${{ secrets.AWS_ACCOUNT_ID }}" \
          --arg region "${{ env.AWS_REGION }}" \
          --arg issuer "$ISSUER_ID" \
          '{
            Version: "2012-10-17",
            Statement: [
              {
                Effect: "Allow",
                Principal: {
                  Federated: ("arn:aws:iam::" + $acct + ":oidc-provider/oidc.eks." + $region + ".amazonaws.com/id/" + $issuer)
                },
                Action: "sts:AssumeRoleWithWebIdentity",
                Condition: {
                  StringEquals: {
                    (
                      "oidc.eks." + $region + ".amazonaws.com/id/" + $issuer + ":sub"
                    ): "system:serviceaccount:kube-system:ebs-csi-controller-sa",
                    (
                      "oidc.eks." + $region + ".amazonaws.com/id/" + $issuer + ":aud"
                    ): "sts.amazonaws.com"
                  }
                }
              }
            ]
          }' > ebs-csi-trust-policy.json

        # Create the IAM role
        aws iam create-role \
          --role-name $EBS_CSI_ROLE_NAME \
          --assume-role-policy-document file://ebs-csi-trust-policy.json || echo "Role may already exist"

        # Attach the EBS CSI Driver policy
        aws iam attach-role-policy \
          --role-name $EBS_CSI_ROLE_NAME \
          --policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy || echo "Policy may already be attached"

        # Get the role ARN
        EBS_CSI_ROLE_ARN=$(aws iam get-role --role-name $EBS_CSI_ROLE_NAME --query "Role.Arn" --output text)
        echo "EBS_CSI_ROLE_ARN=$EBS_CSI_ROLE_ARN" >> $GITHUB_ENV
        echo "🔎 Using IRSA role for add-on: $EBS_CSI_ROLE_ARN"

        # Prefer EKS managed add-on for EBS CSI Driver
        echo "📦 Installing EBS CSI Driver via EKS managed add-on..."
        set +e
        aws eks describe-addon \
          --cluster-name ${{ env.CLUSTER_NAME }} \
          --addon-name aws-ebs-csi-driver \
          --region ${{ env.AWS_REGION }} >/dev/null 2>&1
        ADDON_EXISTS=$?
        set -e
        if [ $ADDON_EXISTS -eq 0 ]; then
          echo "🔄 Updating existing EBS CSI Driver add-on..."
          aws eks update-addon \
            --cluster-name ${{ env.CLUSTER_NAME }} \
            --addon-name aws-ebs-csi-driver \
            --resolve-conflicts OVERWRITE \
            --region ${{ env.AWS_REGION }} \
            --service-account-role-arn "$EBS_CSI_ROLE_ARN" || true
        else
          echo "🆕 Creating EBS CSI Driver add-on..."
          aws eks create-addon \
            --cluster-name ${{ env.CLUSTER_NAME }} \
            --addon-name aws-ebs-csi-driver \
            --resolve-conflicts OVERWRITE \
            --region ${{ env.AWS_REGION }} \
            --service-account-role-arn "$EBS_CSI_ROLE_ARN"
        fi

        # Wait for add-on to become ACTIVE (extend to ~10 minutes)
        echo "⏳ Waiting for EBS CSI Driver add-on to become ACTIVE..."
        for i in {1..60}; do
          STATUS=$(aws eks describe-addon \
            --cluster-name ${{ env.CLUSTER_NAME }} \
            --addon-name aws-ebs-csi-driver \
            --query 'addon.status' \
            --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "UNKNOWN")
          echo "Add-on status: $STATUS"
          if [ "$STATUS" = "ACTIVE" ]; then
            break
          fi
          sleep 10
        done
        FINAL_STATUS=$(aws eks describe-addon \
          --cluster-name ${{ env.CLUSTER_NAME }} \
          --addon-name aws-ebs-csi-driver \
          --query 'addon.status' \
          --output text \
          --region ${{ env.AWS_REGION }} 2>/dev/null || echo "UNKNOWN")
        if [ "$FINAL_STATUS" != "ACTIVE" ]; then
          echo "❌ EBS CSI Driver add-on did not reach ACTIVE (status=$FINAL_STATUS)."
          echo "🔎 Add-on health details:"
          aws eks describe-addon \
            --cluster-name ${{ env.CLUSTER_NAME }} \
            --addon-name aws-ebs-csi-driver \
            --query 'addon.health' \
            --output json \
            --region ${{ env.AWS_REGION }} || true
          echo "🔎 Recent add-on events:" || true
          kubectl get events -n kube-system --sort-by=.lastTimestamp | tail -n 50 || true
          echo "🔎 DaemonSets in kube-system:" && kubectl get ds -n kube-system -o wide || true
          echo "🔎 Pods for ebs-csi-controller:" && kubectl get pods -n kube-system -l app=ebs-csi-controller -o wide || true
          exit 1
        fi

        # Verify installation
        echo "🔍 Verifying EBS CSI Driver installation..."
        kubectl get pods -n kube-system -l app=ebs-csi-controller || true
        kubectl get daemonset -n kube-system | grep -i ebs || true
        kubectl get csidriver ebs.csi.aws.com

        echo "✅ EBS CSI Driver installed successfully!"

    - name: Skip IAM Resources (Using Manual OIDC Role)
      id: iam
      run: |
        echo "ℹ️ Skipping IAM deployment - using manually created OIDC role"
        echo "Role ARN: ${{ secrets.AWS_ROLE_ARN }}"

    - name: Deploy Monitoring Resources
      id: monitoring
      run: |
        cd devops/terraform/03_monitoring
        terraform init -reconfigure \
          -backend-config="bucket=${{ env.BUCKET_NAME }}" \
          -backend-config="key=${{ env.ENVIRONMENT }}/monitoring/terraform.tfstate" \
          -backend-config="region=${{ env.AWS_REGION }}" \
          -backend-config="encrypt=true"
        terraform apply -auto-approve \
          -var="aws_region=${{ env.AWS_REGION }}" \
          -var="project_name=${{ env.PROJECT_NAME }}" \
          -var="environment=${{ env.ENVIRONMENT }}"

    - name: Verify EKS Deployment
      id: verify_eks
      run: |
        echo "🔍 Verifying EKS deployment..."

        # Wait for cluster to be active
        echo "⏳ Waiting for cluster to be active..."
        aws eks wait cluster-active --name ${{ env.CLUSTER_NAME }}

        # Get cluster endpoint
        CLUSTER_ENDPOINT=$(aws eks describe-cluster \
          --name ${{ env.CLUSTER_NAME }} \
          --query "cluster.endpoint" \
          --output text)

        # Get cluster status
        CLUSTER_STATUS=$(aws eks describe-cluster \
          --name ${{ env.CLUSTER_NAME }} \
          --query "cluster.status" \
          --output text)

        echo "CLUSTER_ENDPOINT=$CLUSTER_ENDPOINT" >> $GITHUB_ENV
        echo "CLUSTER_STATUS=$CLUSTER_STATUS" >> $GITHUB_ENV

        if [ "$CLUSTER_STATUS" = "ACTIVE" ]; then
          echo "✅ EKS cluster is active"
          echo "CLUSTER_VERIFIED=true" >> $GITHUB_ENV
        else
          echo "❌ EKS cluster is not active"
          echo "CLUSTER_VERIFIED=false" >> $GITHUB_ENV
          exit 1
        fi

    - name: Verify Network Resources
      id: verify_network
      run: |
        echo "🔍 Verifying network resources..."

        # Check VPC
        VPC_ID=$(aws ec2 describe-vpcs \
          --filters "Name=tag:Project,Values=${{ env.PROJECT_NAME }}" \
                   "Name=tag:Environment,Values=${{ env.ENVIRONMENT }}" \
          --query "Vpcs[0].VpcId" \
          --output text)

        if [ "$VPC_ID" != "None" ]; then
          echo "✅ VPC found: $VPC_ID"
        else
          echo "❌ VPC not found"
          exit 1
        fi

        # Check subnets
        SUBNET_COUNT=$(aws ec2 describe-subnets \
          --filters "Name=vpc-id,Values=$VPC_ID" \
          --query "length(Subnets)")

        if [ "$SUBNET_COUNT" -ge 4 ]; then
          echo "✅ Found $SUBNET_COUNT subnets"
        else
          echo "❌ Expected at least 4 subnets, found $SUBNET_COUNT"
          exit 1
        fi

    - name: Verify Monitoring Resources
      id: verify_monitoring
      run: |
        echo "🔍 Verifying monitoring resources..."

        # Check CloudWatch Dashboard with correct naming pattern
        DASHBOARD_NAME="${{ env.PROJECT_NAME }}-${{ env.ENVIRONMENT }}-${{ env.CLUSTER_NAME }}-dashboard"
        echo "🔍 Looking for dashboard: $DASHBOARD_NAME"

        DASHBOARD=$(aws cloudwatch get-dashboard \
          --dashboard-name "$DASHBOARD_NAME" \
          --query "DashboardBody" \
          --output text 2>/dev/null || echo "None")

        if [ "$DASHBOARD" != "None" ]; then
          echo "✅ CloudWatch Dashboard found: $DASHBOARD_NAME"
        else
          echo "❌ CloudWatch Dashboard not found: $DASHBOARD_NAME"
          
          # List all dashboards for debugging
          echo "🔍 Available dashboards:"
          aws cloudwatch list-dashboards --query "DashboardEntries[*].DashboardName" --output table || true
          exit 1
        fi

        # Check CloudWatch Alarms with better error handling
        echo "🔍 Checking CloudWatch Alarms..."
        ALARM_COUNT=$(aws cloudwatch describe-alarms \
          --query "length(MetricAlarms[?contains(Tags[?Key=='Project'].Value, '${{ env.PROJECT_NAME }}') && contains(Tags[?Key=='Environment'].Value, '${{ env.ENVIRONMENT }}')])" \
          --output text 2>/dev/null || echo "0")

        if [ "$ALARM_COUNT" -gt 0 ]; then
          echo "✅ Found $ALARM_COUNT CloudWatch Alarms"
        else
          echo "⚠️ No CloudWatch Alarms found with tags"
          
          # Try alternative search by name pattern
          PATTERN_ALARMS=$(aws cloudwatch describe-alarms \
            --query "MetricAlarms[?contains(AlarmName, '${{ env.ENVIRONMENT }}-${{ env.PROJECT_NAME }}-${{ env.CLUSTER_NAME }}')]" \
            --output text 2>/dev/null || echo "")
          
          if [ ! -z "$PATTERN_ALARMS" ] && [ "$PATTERN_ALARMS" != "None" ]; then
            echo "✅ Found alarms by name pattern"
          else
            echo "❌ No CloudWatch Alarms found"
            echo "🔍 Available alarms:"
            aws cloudwatch describe-alarms --query "MetricAlarms[*].AlarmName" --output table || true
            exit 1
          fi
        fi

    - name: Export EKS Details
      id: eks_output
      if: success()
      run: |
        ENDPOINT=$(aws eks describe-cluster \
          --name ${{ env.CLUSTER_NAME }} \
          --query "cluster.endpoint" \
          --output text)
        echo "cluster_endpoint=${ENDPOINT}" >> $GITHUB_OUTPUT


    - name: Generate Resource Summary
      id: resource_summary
      if: success()
      run: |
        echo "🔍 Generating comprehensive resource summary..."

        # Get VPC ID for filtering
        VPC_ID=$(aws ec2 describe-vpcs \
          --filters "Name=tag:Project,Values=${{ env.PROJECT_NAME }}" \
                   "Name=tag:Environment,Values=${{ env.ENVIRONMENT }}" \
          --query "Vpcs[0].VpcId" \
          --output text)

        if [ "$VPC_ID" = "None" ] || [ -z "$VPC_ID" ]; then
          echo "❌ VPC not found for resource counting"
          exit 1
        fi

        echo "📊 VPC ID: $VPC_ID"

        # Count Subnets
        SUBNET_COUNT=$(aws ec2 describe-subnets \
          --filters "Name=vpc-id,Values=$VPC_ID" \
          --query "length(Subnets)" \
          --output text)

        # Count Route Tables
        RT_COUNT=$(aws ec2 describe-route-tables \
          --filters "Name=vpc-id,Values=$VPC_ID" \
          --query "length(RouteTables)" \
          --output text)

        # Count Internet Gateways
        IGW_COUNT=$(aws ec2 describe-internet-gateways \
          --filters "Name=attachment.vpc-id,Values=$VPC_ID" \
          --query "length(InternetGateways)" \
          --output text)

        # Count VPC Endpoints
        ENDPOINT_COUNT=$(aws ec2 describe-vpc-endpoints \
          --filters "Name=vpc-id,Values=$VPC_ID" \
          --query "length(VpcEndpoints)" \
          --output text)

        # Count Security Groups
        SG_COUNT=$(aws ec2 describe-security-groups \
          --filters "Name=vpc-id,Values=$VPC_ID" \
          --query "length(SecurityGroups)" \
          --output text)

        # Count NAT Gateways
        NAT_COUNT=$(aws ec2 describe-nat-gateways \
          --filter "Name=vpc-id,Values=$VPC_ID" \
          --query "length(NatGateways)" \
          --output text)

        # Count EIPs
        EIP_COUNT=$(aws ec2 describe-addresses \
          --filters "Name=tag:Project,Values=${{ env.PROJECT_NAME }}" \
                   "Name=tag:Environment,Values=${{ env.ENVIRONMENT }}" \
          --query "length(Addresses)" \
          --output text)

        # Count Network ACLs
        NACL_COUNT=$(aws ec2 describe-network-acls \
          --filters "Name=vpc-id,Values=$VPC_ID" \
          --query "length(NetworkAcls)" \
          --output text)

        # Count EKS Clusters
        EKS_COUNT=$(aws eks list-clusters \
          --query "length(clusters[?@ && contains(@, '${{ env.CLUSTER_NAME }}')])" \
          --output text)

        # Count CloudWatch Dashboards
        DASHBOARD_COUNT=$(aws cloudwatch list-dashboards \
          --query "length(DashboardEntries[?DashboardName && contains(DashboardName, '${{ env.PROJECT_NAME }}') && contains(DashboardName, '${{ env.ENVIRONMENT }}')])" \
          --output text)

        # Count CloudWatch Alarms  
        ALARM_COUNT=$(aws cloudwatch describe-alarms \
          --query "length(MetricAlarms[?Tags && length(Tags[?Key=='Project' && Value=='${{ env.PROJECT_NAME }}']) > \`0\` && length(Tags[?Key=='Environment' && Value=='${{ env.ENVIRONMENT }}']) > \`0\`])" \
          --output text)

        # Store counts in environment variables
        echo "SUBNET_COUNT=$SUBNET_COUNT" >> $GITHUB_ENV
        echo "RT_COUNT=$RT_COUNT" >> $GITHUB_ENV
        echo "IGW_COUNT=$IGW_COUNT" >> $GITHUB_ENV
        echo "ENDPOINT_COUNT=$ENDPOINT_COUNT" >> $GITHUB_ENV
        echo "SG_COUNT=$SG_COUNT" >> $GITHUB_ENV
        echo "NAT_COUNT=$NAT_COUNT" >> $GITHUB_ENV
        echo "EIP_COUNT=$EIP_COUNT" >> $GITHUB_ENV
        echo "NACL_COUNT=$NACL_COUNT" >> $GITHUB_ENV
        echo "EKS_COUNT=$EKS_COUNT" >> $GITHUB_ENV
        echo "DASHBOARD_COUNT=$DASHBOARD_COUNT" >> $GITHUB_ENV
        echo "ALARM_COUNT=$ALARM_COUNT" >> $GITHUB_ENV

        # Calculate total resources
        TOTAL_RESOURCES=$((SUBNET_COUNT + RT_COUNT + IGW_COUNT + ENDPOINT_COUNT + SG_COUNT + NAT_COUNT + EIP_COUNT + NACL_COUNT + EKS_COUNT + DASHBOARD_COUNT + ALARM_COUNT))
        echo "TOTAL_RESOURCES=$TOTAL_RESOURCES" >> $GITHUB_ENV

        echo "✅ Resource counting completed!"

    - name: Display Resource Summary
      if: success()
      run: |
        echo "### 🏗️ Infrastructure Deployment Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Environment:** \`${{ env.ENVIRONMENT }}\`" >> $GITHUB_STEP_SUMMARY
        echo "**Project:** \`${{ env.PROJECT_NAME }}\`" >> $GITHUB_STEP_SUMMARY
        echo "**Region:** \`${{ env.AWS_REGION }}\`" >> $GITHUB_STEP_SUMMARY
        echo "**Timestamp:** \`$(date -u '+%Y-%m-%d %H:%M:%S UTC')\`" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        echo "### 📊 Resource Counts" >> $GITHUB_STEP_SUMMARY
        echo "| Resource Type | Count | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|---------------|-------|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| **VPC** | 1 | ✅ |" >> $GITHUB_STEP_SUMMARY
        echo "| **Subnets** | ${{ env.SUBNET_COUNT }} | ✅ |" >> $GITHUB_STEP_SUMMARY
        echo "| **Route Tables** | ${{ env.RT_COUNT }} | ✅ |" >> $GITHUB_STEP_SUMMARY
        echo "| **Internet Gateways** | ${{ env.IGW_COUNT }} | ✅ |" >> $GITHUB_STEP_SUMMARY
        echo "| **VPC Endpoints** | ${{ env.ENDPOINT_COUNT }} | ✅ |" >> $GITHUB_STEP_SUMMARY
        echo "| **Security Groups** | ${{ env.SG_COUNT }} | ✅ |" >> $GITHUB_STEP_SUMMARY
        echo "| **NAT Gateways** | ${{ env.NAT_COUNT }} | ✅ |" >> $GITHUB_STEP_SUMMARY
        echo "| **Elastic IPs** | ${{ env.EIP_COUNT }} | ✅ |" >> $GITHUB_STEP_SUMMARY
        echo "| **Network ACLs** | ${{ env.NACL_COUNT }} | ✅ |" >> $GITHUB_STEP_SUMMARY
        echo "| **EKS Clusters** | ${{ env.EKS_COUNT }} | ✅ |" >> $GITHUB_STEP_SUMMARY
        echo "| **CloudWatch Dashboards** | ${{ env.DASHBOARD_COUNT }} | ✅ |" >> $GITHUB_STEP_SUMMARY
        echo "| **CloudWatch Alarms** | ${{ env.ALARM_COUNT }} | ✅ |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        echo "### 🔗 EKS Cluster Details" >> $GITHUB_STEP_SUMMARY
        echo "| Property | Value |" >> $GITHUB_STEP_SUMMARY
        echo "|----------|-------|" >> $GITHUB_STEP_SUMMARY
        echo "| **Cluster Name** | \`${{ env.CLUSTER_NAME }}\` |" >> $GITHUB_STEP_SUMMARY
        echo "| **Cluster Status** | \`${{ env.CLUSTER_STATUS }}\` |" >> $GITHUB_STEP_SUMMARY
        echo "| **Cluster Endpoint** | \`${{ env.CLUSTER_ENDPOINT }}\` |" >> $GITHUB_STEP_SUMMARY
        echo "| **Kubernetes Version** | \`1.31\` |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        echo "### 🎯 Deployment Status" >> $GITHUB_STEP_SUMMARY
        if [ "${{ env.CLUSTER_VERIFIED }}" = "true" ]; then
          echo "**Status:** ✅ **SUCCESS** - All resources deployed successfully!" >> $GITHUB_STEP_SUMMARY
        else
          echo "**Status:** ❌ **FAILED** - Some resources failed to deploy" >> $GITHUB_STEP_SUMMARY
        fi
        echo "" >> $GITHUB_STEP_SUMMARY

        echo "**Configuration:**" >> $GITHUB_STEP_SUMMARY
        echo "- Terraform Version: \`${{ env.TF_VERSION }}\`" >> $GITHUB_STEP_SUMMARY
        echo "- AWS Region: \`${{ env.AWS_REGION }}\`" >> $GITHUB_STEP_SUMMARY
        echo "- Project: \`${{ env.PROJECT_NAME }}\`" >> $GITHUB_STEP_SUMMARY
