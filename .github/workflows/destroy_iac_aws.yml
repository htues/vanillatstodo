name: Destroy TFIaC on AWS

"on":
  workflow_dispatch:
    inputs:
      confirmation:
        description: "Type 'destroy' to confirm"
        required: true
        type: string
      environment:
        description: "Environment to destroy (staging/production/experimental)"
        required: true
        type: choice
        options:
        - staging
        - production
        - experimental
        default: "staging"

permissions:
  contents: read
  id-token: write

env:
  CLUSTER_NAME: "vanillatstodo-cluster"
  BUCKET_NAME: "vanillatstodo-terraform-state"
  AWS_REGION: "us-east-2"
  TF_VERSION: "1.10.0"
  PROJECT_NAME: "vanillatstodo"
  ENVIRONMENT: ${{ github.event.inputs.environment }}

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

jobs:
  destroy:
    timeout-minutes: 30
    name: "Destroy Infrastructure"
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.confirmation == 'destroy' }}

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v3

    - name: Debug OIDC token claims
      uses: actions/github-script@v7
      with:
        script: |
          const token = await core.getIDToken('sts.amazonaws.com');
          const payload = JSON.parse(Buffer.from(token.split('.')[1], 'base64').toString());
          core.info(`aud=${payload.aud}`);
          core.info(`sub=${payload.sub}`);
          core.info(`iss=${payload.iss}`);
          core.info(`Full payload: ${JSON.stringify(payload, null, 2)}`);

    - name: Configure AWS Credentials (OIDC)
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v2
      with:
        terraform_version: ${{ env.TF_VERSION }}

    - name: Use Manual IAM Role ARN
      id: iam_role_arn
      run: |
        echo "github_actions_role_arn=${{ secrets.AWS_ROLE_ARN }}" >> $GITHUB_OUTPUT
        echo "âœ… Using Manual GitHub Actions Role ARN: ${{ secrets.AWS_ROLE_ARN }}"

    - name: Destroy State Resources
      id: state
      run: |
        cd devops/terraform/00_state
        terraform init -reconfigure \
          -backend-config="bucket=${{ env.BUCKET_NAME }}" \
          -backend-config="key=${{ env.ENVIRONMENT }}/state/terraform.tfstate" \
          -backend-config="region=${{ env.AWS_REGION }}" \
          -backend-config="encrypt=true"
        terraform destroy -auto-approve \
          -var="aws_region=${{ env.AWS_REGION }}" \
          -var="project_name=${{ env.PROJECT_NAME }}" \
          -var="environment=${{ env.ENVIRONMENT }}"

    - name: Destroy EKS Resources
      id: eks_cleanup
      run: |
        cd devops/terraform/02_eks
        terraform init -reconfigure \
          -backend-config="bucket=${{ env.BUCKET_NAME }}" \
          -backend-config="key=${{ env.ENVIRONMENT }}/eks/terraform.tfstate" \
          -backend-config="region=${{ env.AWS_REGION }}" \
          -backend-config="encrypt=true"

        # Check if network state exists and has outputs
        echo "ðŸ” Checking network state availability..."
        NETWORK_STATE_EXISTS=$(aws s3api head-object \
          --bucket ${{ env.BUCKET_NAME }} \
          --key ${{ env.ENVIRONMENT }}/network/terraform.tfstate 2>/dev/null && echo "true" || echo "false")

        echo "ðŸ“Š Network state exists: $NETWORK_STATE_EXISTS"

        if [ "$NETWORK_STATE_EXISTS" = "false" ]; then
          echo "âš ï¸ Network state not found. Checking if EKS resources exist directly..."
          
          # Check if EKS cluster exists directly
          CLUSTER_EXISTS=$(aws eks describe-cluster \
            --name ${{ env.CLUSTER_NAME }} \
            --query 'cluster.name' \
            --output text 2>/dev/null || echo "false")
          
          echo "ðŸ“Š EKS cluster exists: $CLUSTER_EXISTS"
          
          if [ "$CLUSTER_EXISTS" = "${{ env.CLUSTER_NAME }}" ]; then
            echo "âš ï¸ EKS cluster exists but network state is missing. Attempting direct cleanup..."
            
            # Try to destroy with -refresh=false to skip state reading
            terraform destroy -auto-approve -refresh=false \
              -var="cluster_name=${{ env.CLUSTER_NAME }}" \
              -var="aws_region=${{ env.AWS_REGION }}" \
              -var="project_name=${{ env.PROJECT_NAME }}" \
              -var="environment=${{ env.ENVIRONMENT }}" \
              -var="cluster_role_name=${{ env.ENVIRONMENT == 'production' && 'production-vanillatstodo-cluster-role' || 'staging-vanillatstodo-cluster-role' }}" || true
          else
            echo "âœ… EKS cluster not found. Skipping EKS destruction."
          fi
        else
          echo "âœ… Network state found. Checking if it contains required outputs..."
          
          # Download and check network state content
          aws s3 cp s3://${{ env.BUCKET_NAME }}/${{ env.ENVIRONMENT }}/network/terraform.tfstate /tmp/network_state.json
          
          # Check if state has outputs
          HAS_OUTPUTS=$(jq -r '.outputs // empty' /tmp/network_state.json 2>/dev/null || echo "false")
          HAS_VPC_ID=$(jq -r '.outputs.vpc_id.value // empty' /tmp/network_state.json 2>/dev/null || echo "false")
          
          echo "ðŸ“Š Network state has outputs: $HAS_OUTPUTS"
          echo "ðŸ“Š Network state has vpc_id: $HAS_VPC_ID"
          
          if [ "$HAS_OUTPUTS" = "false" ] || [ "$HAS_VPC_ID" = "false" ] || [ "$HAS_VPC_ID" = "null" ] || [ "$HAS_VPC_ID" = "" ]; then
            echo "âš ï¸ Network state exists but is empty or missing required outputs. Skipping Terraform EKS destruction."
            echo "ðŸ”„ Will rely on manual EKS cleanup step instead."
            echo "âœ… EKS Terraform destruction skipped successfully."
            # Set a flag to indicate we're skipping Terraform EKS destruction
            echo "SKIP_TERRAFORM_EKS=true" >> $GITHUB_ENV
          else
            echo "âœ… Network state has required outputs. Proceeding with normal EKS destruction..."
            echo "SKIP_TERRAFORM_EKS=false" >> $GITHUB_ENV
            
            # Check for and force unlock any stale locks
            echo "ðŸ”“ Checking for stale state locks..."
            if terraform plan -detailed-exitcode >/dev/null 2>&1; then
              echo "âœ… No state lock detected"
            else
              echo "âš ï¸ State lock detected, attempting to force unlock..."
              # Try to get the lock ID from the error message
              LOCK_ID=$(terraform plan 2>&1 | grep -o 'ID: [a-f0-9-]*' | cut -d' ' -f2 || echo "")
              if [ ! -z "$LOCK_ID" ]; then
                echo "ðŸ”“ Force unlocking with ID: $LOCK_ID"
                terraform force-unlock -force "$LOCK_ID"
              else
                echo "âš ï¸ Could not determine lock ID, trying generic unlock..."
                terraform force-unlock -force f2e10d17-e3da-6e08-aef1-7a64455b038d || true
              fi
            fi
            
            terraform destroy -auto-approve \
              -var="cluster_name=${{ env.CLUSTER_NAME }}" \
              -var="aws_region=${{ env.AWS_REGION }}" \
              -var="project_name=${{ env.PROJECT_NAME }}" \
              -var="environment=${{ env.ENVIRONMENT }}" \
              -var="cluster_role_name=${{ env.ENVIRONMENT == 'production' && 'production-vanillatstodo-cluster-role' || 'staging-vanillatstodo-cluster-role' }}"
          fi
        fi

    - name: Clean Kubernetes-Managed AWS Resources
      id: k8s_resources_cleanup
      run: |
        echo "ðŸ§¹ Cleaning Kubernetes-managed AWS resources..."

        # Check if EKS cluster exists and is accessible
        if aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} >/dev/null 2>&1; then
          echo "âœ… EKS cluster found, attempting to clean Kubernetes resources..."
          
          # Install kubectl
          echo "ðŸ“¦ Installing kubectl..."
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/
          
          # Update kubeconfig
          echo "ðŸ”§ Configuring kubectl..."
          aws eks update-kubeconfig --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
          
          # Test cluster connectivity
          if kubectl cluster-info >/dev/null 2>&1; then
            echo "âœ… Cluster connection successful"
            
            # 1. Delete LoadBalancer services (creates AWS LoadBalancers)
            echo "ðŸ—‘ï¸ Deleting LoadBalancer services..."
            LB_SERVICES=$(kubectl get services --all-namespaces --field-selector spec.type=LoadBalancer -o name 2>/dev/null || echo "")
            if [ ! -z "$LB_SERVICES" ]; then
              echo "   Found LoadBalancer services: $LB_SERVICES"
              kubectl delete services --all-namespaces --field-selector spec.type=LoadBalancer --timeout=120s || true
              echo "   â³ Waiting for LoadBalancers to be deleted..."
              sleep 60
            else
              echo "   â„¹ï¸ No LoadBalancer services found"
            fi
            
            # 2. Delete Ingresses (may create AWS LoadBalancers/ALBs)
            echo "ðŸ—‘ï¸ Deleting Ingresses..."
            INGRESSES=$(kubectl get ingress --all-namespaces -o name 2>/dev/null || echo "")
            if [ ! -z "$INGRESSES" ]; then
              echo "   Found ingresses: $INGRESSES"
              kubectl delete ingress --all-namespaces --timeout=120s || true
              echo "   â³ Waiting for Ingress resources to be deleted..."
              sleep 30
            else
              echo "   â„¹ï¸ No Ingresses found"
            fi
            
            # 3. Delete PersistentVolumes (creates AWS EBS volumes)
            echo "ðŸ—‘ï¸ Deleting PersistentVolumeClaims..."
            PVCS=$(kubectl get pvc --all-namespaces -o name 2>/dev/null || echo "")
            if [ ! -z "$PVCS" ]; then
              echo "   Found PVCs: $PVCS"
              kubectl delete pvc --all-namespaces --timeout=120s || true
              echo "   â³ Waiting for PVCs to be deleted..."
              sleep 30
            else
              echo "   â„¹ï¸ No PVCs found"
            fi
            
            # 4. Delete any remaining resources in our application namespace
            echo "ðŸ—‘ï¸ Cleaning application resources..."
            kubectl delete deployment,service,configmap,secret,ingress -l app=vanillatstodo --timeout=60s || true
            
            echo "âœ… Kubernetes resource cleanup completed"
            echo "K8S_CLEANUP_STATUS=âœ…" >> $GITHUB_ENV
            
          else
            echo "âš ï¸ Cannot connect to cluster, skipping Kubernetes cleanup"
            echo "â„¹ï¸ Resources may need manual cleanup in AWS Console"
            echo "K8S_CLEANUP_STATUS=âš ï¸" >> $GITHUB_ENV
          fi
        else
          echo "â„¹ï¸ EKS cluster not found, skipping Kubernetes cleanup"
          echo "K8S_CLEANUP_STATUS=âœ…" >> $GITHUB_ENV
        fi

    - name: Manual EKS Cleanup (if needed)
      id: manual_eks_cleanup
      run: |
        echo "ðŸ” Checking for any remaining EKS resources..."

        # Check for EKS clusters
        CLUSTERS=$(aws eks list-clusters --query "clusters[?contains(@, '${{ env.CLUSTER_NAME }}')]" --output text)
        if [ ! -z "$CLUSTERS" ] && [ "$CLUSTERS" != "None" ]; then
          echo "âš ï¸ Found EKS clusters: $CLUSTERS"
          
          for cluster in $CLUSTERS; do
            echo "ðŸ—‘ï¸ Deleting EKS cluster: $cluster"
            
            # Delete node groups first
            NODE_GROUPS=$(aws eks list-nodegroups --cluster-name $cluster --query "nodegroups" --output text 2>/dev/null || echo "")
            if [ ! -z "$NODE_GROUPS" ] && [ "$NODE_GROUPS" != "None" ]; then
              echo "ðŸ—‘ï¸ Deleting node groups for cluster $cluster: $NODE_GROUPS"
              for ng in $NODE_GROUPS; do
                aws eks delete-nodegroup --cluster-name $cluster --nodegroup-name $ng || true
              done
              
              # Wait for node groups to be deleted
              echo "â³ Waiting for node groups to be deleted..."
              sleep 60
            fi
            
            # Delete the cluster
            aws eks delete-cluster --name $cluster || true
          done
        else
          echo "âœ… No EKS clusters found"
        fi

        # Check for EKS-related security groups
        echo "ðŸ” Checking for EKS-related security groups..."
        EKS_SGS=$(aws ec2 describe-security-groups \
          --filters "Name=group-name,Values=*eks*" "Name=tag:Project,Values=${{ env.PROJECT_NAME }}" \
          --query 'SecurityGroups[*].GroupId' \
          --output text)

        if [ ! -z "$EKS_SGS" ] && [ "$EKS_SGS" != "None" ]; then
          echo "ðŸ—‘ï¸ Found EKS security groups: $EKS_SGS"
          for sg in $EKS_SGS; do
            echo "ðŸ—‘ï¸ Deleting security group: $sg"
            aws ec2 delete-security-group --group-id $sg || true
          done
        fi

        # Check for EKS-related VPC endpoints
        echo "ðŸ” Checking for EKS-related VPC endpoints..."
        VPC_IDS=$(aws ec2 describe-vpcs \
          --filters "Name=tag:Name,Values=*vanillatstodo*" \
          --query 'Vpcs[*].VpcId' \
          --output text)

        for VPC_ID in $VPC_IDS; do
          EKS_ENDPOINTS=$(aws ec2 describe-vpc-endpoints \
            --filters "Name=vpc-id,Values=$VPC_ID" "Name=service-name,Values=com.amazonaws.*.eks" \
            --query 'VpcEndpoints[*].VpcEndpointId' \
            --output text)
          
          if [ ! -z "$EKS_ENDPOINTS" ] && [ "$EKS_ENDPOINTS" != "None" ]; then
            echo "ðŸ—‘ï¸ Found EKS VPC endpoints: $EKS_ENDPOINTS"
            aws ec2 delete-vpc-endpoints --vpc-endpoint-ids $EKS_ENDPOINTS || true
          fi
        done
                  
        # Wait a bit for resources to be fully deleted
        echo "â³ Waiting for EKS resources to be fully deleted..."
        sleep 30

    - name: Wait for EKS Resources to be Fully Deleted
      id: wait_for_eks_cleanup
      run: |
        echo "â³ Waiting for EKS resources to be fully deleted..."
        sleep 60

        # Check if there are any remaining EKS-related resources
        echo "ðŸ” Checking for remaining EKS resources..."

        # Check for EKS clusters
        CLUSTERS=$(aws eks list-clusters --query "clusters[?contains(@, '${{ env.CLUSTER_NAME }}')]" --output text)
        if [ ! -z "$CLUSTERS" ] && [ "$CLUSTERS" != "None" ]; then
          echo "âš ï¸ Found remaining EKS clusters: $CLUSTERS"
          echo "â³ Waiting additional 2 minutes for cluster deletion..."
          sleep 120
        else
          echo "âœ… No remaining EKS clusters found"
        fi

        # Check for EKS node groups
        NODE_GROUPS=$(aws eks list-nodegroups --cluster-name ${{ env.CLUSTER_NAME }} --query "nodegroups" --output text 2>/dev/null || echo "")
        if [ ! -z "$NODE_GROUPS" ] && [ "$NODE_GROUPS" != "None" ]; then
          echo "âš ï¸ Found remaining EKS node groups: $NODE_GROUPS"
          echo "â³ Waiting additional 2 minutes for node group deletion..."
          sleep 120
        else
          echo "âœ… No remaining EKS node groups found"
        fi

    - name: Destroy Monitoring Resources
      id: monitoring_cleanup
      run: |
        cd devops/terraform/03_monitoring
        terraform init -reconfigure \
          -backend-config="bucket=${{ env.BUCKET_NAME }}" \
          -backend-config="key=${{ env.ENVIRONMENT }}/monitoring/terraform.tfstate" \
          -backend-config="region=${{ env.AWS_REGION }}" \
          -backend-config="encrypt=true"

        # Check if EKS cluster still exists before attempting Terraform destroy
        echo "ðŸ” Checking if EKS cluster exists before monitoring destruction..."
        CLUSTER_EXISTS=$(aws eks describe-cluster \
          --name ${{ env.CLUSTER_NAME }} \
          --query 'cluster.name' \
          --output text 2>/dev/null || echo "false")

        echo "ðŸ“Š EKS cluster exists: $CLUSTER_EXISTS"

        if [ "$CLUSTER_EXISTS" = "${{ env.CLUSTER_NAME }}" ]; then
          echo "âœ… EKS cluster exists. Proceeding with normal Terraform destroy..."
          terraform destroy -auto-approve \
            -var="cluster_name=${{ env.CLUSTER_NAME }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="project_name=${{ env.PROJECT_NAME }}" \
            -var="environment=${{ env.ENVIRONMENT }}"
        else
          echo "âš ï¸ EKS cluster not found. Using refresh=false to avoid data source errors..."
          
          # Try to destroy without refreshing state (skip data source validation)
          terraform destroy -auto-approve -refresh=false \
            -var="cluster_name=${{ env.CLUSTER_NAME }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="project_name=${{ env.PROJECT_NAME }}" \
            -var="environment=${{ env.ENVIRONMENT }}" || {
            echo "âš ï¸ Terraform destroy failed due to missing EKS cluster. Proceeding with manual cleanup..."
            echo "MONITORING_TERRAFORM_FAILED=true" >> $GITHUB_ENV
          }
        fi

    - name: Enhanced Network Dependency Cleanup
      id: enhanced_network_cleanup
      run: |
        echo "ðŸ” Performing enhanced network dependency cleanup..."

        # Find all VPCs with our project tags
        VPC_IDS=$(aws ec2 describe-vpcs \
          --filters "Name=tag:Name,Values=*vanillatstodo*" \
          --query 'Vpcs[*].VpcId' \
          --output text)

        for VPC_ID in $VPC_IDS; do
          echo "ðŸ” Cleaning up dependencies for VPC: $VPC_ID"
          
          # 1. Delete all VPC endpoints
          echo "ðŸ—‘ï¸ Deleting all VPC endpoints..."
          ENDPOINTS=$(aws ec2 describe-vpc-endpoints \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query 'VpcEndpoints[*].VpcEndpointId' \
            --output text)
          
          if [ ! -z "$ENDPOINTS" ] && [ "$ENDPOINTS" != "None" ]; then
            echo "ðŸ—‘ï¸ Found VPC endpoints: $ENDPOINTS"
            aws ec2 delete-vpc-endpoints --vpc-endpoint-ids $ENDPOINTS || true
            sleep 30
          fi
          
          # 2. Delete all network interfaces with proper error handling
          echo "ðŸ—‘ï¸ Deleting all network interfaces..."
          ENI_IDS=$(aws ec2 describe-network-interfaces \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query 'NetworkInterfaces[*].NetworkInterfaceId' \
            --output text)
          
          if [ ! -z "$ENI_IDS" ] && [ "$ENI_IDS" != "None" ]; then
            echo "ðŸ—‘ï¸ Found network interfaces: $ENI_IDS"
            for eni in $ENI_IDS; do
              echo "ðŸ” Checking network interface: $eni"
              # Check if ENI still exists
              if aws ec2 describe-network-interfaces --network-interface-ids $eni >/dev/null 2>&1; then
                ENI_STATUS=$(aws ec2 describe-network-interfaces \
                  --network-interface-ids $eni \
                  --query 'NetworkInterfaces[0].Status' \
                  --output text 2>/dev/null || echo "not-found")
                
                if [ "$ENI_STATUS" = "available" ]; then
                  echo "ðŸ—‘ï¸ Deleting available network interface: $eni"
                  aws ec2 delete-network-interface --network-interface-id $eni || true
                  sleep 3
                elif [ "$ENI_STATUS" = "not-found" ]; then
                  echo "â„¹ï¸ Network interface $eni already deleted"
                else
                  echo "âš ï¸ Network interface $eni is not available (status: $ENI_STATUS)"
                fi
              else
                echo "â„¹ï¸ Network interface $eni no longer exists (already cleaned up)"
              fi
            done
          fi
          
          # 3. Delete all security groups (except default)
          echo "ðŸ—‘ï¸ Deleting all security groups..."
          SG_IDS=$(aws ec2 describe-security-groups \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query 'SecurityGroups[?GroupName!=`default`].GroupId' \
            --output text)
          
          if [ ! -z "$SG_IDS" ] && [ "$SG_IDS" != "None" ]; then
            echo "ðŸ—‘ï¸ Found security groups: $SG_IDS"
            for sg in $SG_IDS; do
              aws ec2 delete-security-group --group-id $sg || true
              sleep 2
            done
          fi
          
          # 4. Delete all route table associations and route tables
          echo "ðŸ—‘ï¸ Deleting route table associations..."
          RT_IDS=$(aws ec2 describe-route-tables \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query 'RouteTables[?Associations[0].Main!=`true`].RouteTableId' \
            --output text)
          
          if [ ! -z "$RT_IDS" ] && [ "$RT_IDS" != "None" ]; then
            echo "ðŸ—‘ï¸ Found route tables: $RT_IDS"
            for rt in $RT_IDS; do
              # Disassociate all associations first
              ASSOC_IDS=$(aws ec2 describe-route-tables \
                --route-table-id $rt \
                --query 'RouteTables[0].Associations[*].RouteTableAssociationId' \
                --output text)
              
              for assoc in $ASSOC_IDS; do
                aws ec2 disassociate-route-table --association-id $assoc || true
              done
              
              # Delete the route table
              aws ec2 delete-route-table --route-table-id $rt || true
              sleep 2
            done
          fi
          
          # 5. Delete all NAT gateways
          echo "ðŸ—‘ï¸ Deleting NAT gateways..."
          NAT_IDS=$(aws ec2 describe-nat-gateways \
            --filter "Name=vpc-id,Values=$VPC_ID" \
            --query 'NatGateways[?State!=`deleted`].NatGatewayId' \
            --output text)
          
          if [ ! -z "$NAT_IDS" ] && [ "$NAT_IDS" != "None" ]; then
            echo "ðŸ—‘ï¸ Found NAT gateways: $NAT_IDS"
            for nat in $NAT_IDS; do
              aws ec2 delete-nat-gateway --nat-gateway-id $nat || true
            done
            
            # Wait for NAT gateways to be deleted
            if [ ! -z "$NAT_IDS" ]; then
              echo "â³ Waiting for NAT gateways to be deleted..."
              sleep 45
            fi
          fi
          
          # 6. Delete all network ACLs (except default)
          echo "ðŸ—‘ï¸ Deleting network ACLs..."
          NACL_IDS=$(aws ec2 describe-network-acls \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query 'NetworkAcls[?!IsDefault].NetworkAclId' \
            --output text)
          
          if [ ! -z "$NACL_IDS" ] && [ "$NACL_IDS" != "None" ]; then
            echo "ðŸ—‘ï¸ Found network ACLs: $NACL_IDS"
            for nacl in $NACL_IDS; do
              aws ec2 delete-network-acl --network-acl-id $nacl || true
              sleep 2
            done
          fi
          
          # 7. Delete all subnets
          echo "ðŸ—‘ï¸ Deleting all subnets..."
          SUBNET_IDS=$(aws ec2 describe-subnets \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query 'Subnets[*].SubnetId' \
            --output text)
          
          if [ ! -z "$SUBNET_IDS" ] && [ "$SUBNET_IDS" != "None" ]; then
            echo "ðŸ—‘ï¸ Found subnets: $SUBNET_IDS"
            for subnet in $SUBNET_IDS; do
              aws ec2 delete-subnet --subnet-id $subnet || true
              sleep 2
            done
          fi
          
          # 8. Delete internet gateway
          echo "ðŸ—‘ï¸ Deleting internet gateway..."
          IGW_ID=$(aws ec2 describe-internet-gateways \
            --filters "Name=attachment.vpc-id,Values=$VPC_ID" \
            --query 'InternetGateways[*].InternetGatewayId' \
            --output text)
          
          if [ ! -z "$IGW_ID" ] && [ "$IGW_ID" != "None" ]; then
            echo "ðŸ—‘ï¸ Found internet gateway: $IGW_ID"
            aws ec2 detach-internet-gateway --internet-gateway-id $IGW_ID --vpc-id $VPC_ID || true
            aws ec2 delete-internet-gateway --internet-gateway-id $IGW_ID || true
            sleep 2
          fi
          
          echo "âœ… Completed dependency cleanup for VPC: $VPC_ID"
        done

        echo "â³ Waiting for all resources to be fully deleted..."
        sleep 30

    - name: Clean AWS Dependencies Before Network Destruction
      id: aws_dependencies_cleanup
      run: |
        echo "ðŸ§¹ Cleaning AWS dependencies that block network destruction..."

        # 1. Delete Load Balancers in the VPC
        echo "ðŸ” Finding Load Balancers..."
        VPC_ID=$(aws ec2 describe-vpcs \
          --filters "Name=tag:Project,Values=${{ env.PROJECT_NAME }}" "Name=tag:Environment,Values=${{ env.ENVIRONMENT }}" \
          --query "Vpcs[0].VpcId" --output text 2>/dev/null || echo "None")

        if [ "$VPC_ID" != "None" ] && [ "$VPC_ID" != "null" ] && [ ! -z "$VPC_ID" ]; then
          echo "âœ… Found VPC: $VPC_ID"
          
          # Delete Classic Load Balancers
          echo "ðŸ—‘ï¸ Deleting Classic Load Balancers..."
          CLASSIC_LBS=$(aws elb describe-load-balancers \
            --query "LoadBalancerDescriptions[?VPCId=='$VPC_ID'].LoadBalancerName" \
            --output text 2>/dev/null || echo "")
          
          if [ ! -z "$CLASSIC_LBS" ] && [ "$CLASSIC_LBS" != "None" ]; then
            for lb in $CLASSIC_LBS; do
              echo "   ðŸ—‘ï¸ Deleting Classic LB: $lb"
              aws elb delete-load-balancer --load-balancer-name "$lb" || true
            done
          fi
          
          # Delete Application/Network Load Balancers
          echo "ðŸ—‘ï¸ Deleting ALB/NLB Load Balancers..."
          ALB_ARNS=$(aws elbv2 describe-load-balancers \
            --query "LoadBalancers[?VpcId=='$VPC_ID'].LoadBalancerArn" \
            --output text 2>/dev/null || echo "")
          
          if [ ! -z "$ALB_ARNS" ] && [ "$ALB_ARNS" != "None" ]; then
            for arn in $ALB_ARNS; do
              echo "   ðŸ—‘ï¸ Deleting ALB/NLB: $arn"
              aws elbv2 delete-load-balancer --load-balancer-arn "$arn" || true
            done
          fi
          
          # Wait for LBs to be deleted
          echo "â³ Waiting for Load Balancers to be deleted..."
          sleep 60
          
          # 2. Delete NAT Gateway EIPs
          echo "ðŸ—‘ï¸ Releasing NAT Gateway Elastic IPs..."
          EIP_ALLOCS=$(aws ec2 describe-addresses \
            --filters "Name=domain,Values=vpc" \
            --query "Addresses[?AssociationId!=null].AllocationId" \
            --output text 2>/dev/null || echo "")
          
          if [ ! -z "$EIP_ALLOCS" ] && [ "$EIP_ALLOCS" != "None" ]; then
            for alloc in $EIP_ALLOCS; do
              echo "   ðŸ—‘ï¸ Releasing EIP: $alloc"
              aws ec2 release-address --allocation-id "$alloc" || true
            done
          fi
          
          # 3. Force delete Network Interfaces
          echo "ðŸ—‘ï¸ Deleting Network Interfaces..."
          ENI_IDS=$(aws ec2 describe-network-interfaces \
            --filters "Name=vpc-id,Values=$VPC_ID" "Name=status,Values=available" \
            --query "NetworkInterfaces[].NetworkInterfaceId" \
            --output text 2>/dev/null || echo "")
          
          if [ ! -z "$ENI_IDS" ] && [ "$ENI_IDS" != "None" ]; then
            for eni in $ENI_IDS; do
              echo "   ðŸ—‘ï¸ Deleting ENI: $eni"
              aws ec2 delete-network-interface --network-interface-id "$eni" || true
            done
          fi
          
          # 4. Delete Security Groups (except default)
          echo "ðŸ—‘ï¸ Deleting Security Groups..."
          SG_IDS=$(aws ec2 describe-security-groups \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query "SecurityGroups[?GroupName!='default'].GroupId" \
            --output text 2>/dev/null || echo "")
          
          if [ ! -z "$SG_IDS" ] && [ "$SG_IDS" != "None" ]; then
            for sg in $SG_IDS; do
              echo "   ðŸ—‘ï¸ Deleting Security Group: $sg"
              aws ec2 delete-security-group --group-id "$sg" || true
            done
          fi
          
          echo "â³ Waiting for dependencies to clear..."
          sleep 30
        else
          echo "â„¹ï¸ No VPC found, skipping dependency cleanup"
        fi

    - name: Destroy Network Resources
      id: network_cleanup
      run: |
        cd devops/terraform/01_network
        terraform init -reconfigure \
          -backend-config="bucket=${{ env.BUCKET_NAME }}" \
          -backend-config="key=${{ env.ENVIRONMENT }}/network/terraform.tfstate" \
          -backend-config="region=${{ env.AWS_REGION }}" \
          -backend-config="encrypt=true"

        # Check for and force unlock any stale locks
        echo "ðŸ”“ Checking for stale state locks..."

        # Try multiple methods to get the lock ID
        LOCK_ID=""

        # Method 1: Try terraform plan to get lock info
        if terraform plan -detailed-exitcode >/dev/null 2>&1; then
          echo "âœ… No state lock detected"
        else
          echo "âš ï¸ State lock detected, extracting lock ID..."
          
          # Method 2: Get lock ID from terraform plan error output
          LOCK_ID=$(terraform plan 2>&1 | grep -E 'ID:\s+[a-f0-9-]+' | head -1 | sed 's/.*ID:\s*//' | tr -d ' ' || echo "")
          
          if [ -z "$LOCK_ID" ]; then
            # Method 3: Try terraform init to get lock info
            LOCK_ID=$(terraform init 2>&1 | grep -E 'ID:\s+[a-f0-9-]+' | head -1 | sed 's/.*ID:\s*//' | tr -d ' ' || echo "")
          fi
          
          if [ -z "$LOCK_ID" ]; then
            # Method 4: Try from any terraform command
            LOCK_ID=$(terraform refresh 2>&1 | grep -E 'ID:\s+[a-f0-9-]+' | head -1 | sed 's/.*ID:\s*//' | tr -d ' ' || echo "")
          fi
          
          if [ ! -z "$LOCK_ID" ]; then
            echo "ðŸ”“ Found lock ID: $LOCK_ID"
            echo "ðŸ”“ Force unlocking..."
            terraform force-unlock -force "$LOCK_ID" || true
          else
            echo "âš ï¸ Could not determine lock ID, trying common patterns..."
            # Try known lock IDs from the error message
            terraform force-unlock -force "26caec4a-58ea-883b-72d5-302b3bd9afcd" || true
            terraform force-unlock -force "22adec0c-b380-16f0-1453-8ae8d9bd834f" || true
          fi
          
          # Verify unlock worked
          echo "ðŸ” Verifying state lock is cleared..."
          sleep 5
        fi

        echo "ðŸš€ Proceeding with network destruction..."

        # Retry logic for network destruction
        for attempt in 1 2 3; do
          echo "ðŸ”„ Network destruction attempt $attempt/3..."
          
          # Use -lock=false on retry attempts if lock issues persist
          LOCK_FLAG=""
          if [ $attempt -gt 1 ]; then
            echo "âš ï¸ Using -lock=false to bypass state locking on retry..."
            LOCK_FLAG="-lock=false"
          fi
          
          if terraform destroy -auto-approve $LOCK_FLAG \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="project_name=${{ env.PROJECT_NAME }}" \
            -var="environment=${{ env.ENVIRONMENT }}"; then
            echo "âœ… Network destruction successful!"
            break
          else
            echo "âŒ Network destruction failed on attempt $attempt"
            
            if [ $attempt -lt 3 ]; then
              echo "â³ Waiting before retry..."
              sleep 60
              
              # Try to clean more dependencies
              echo "ðŸ§¹ Additional dependency cleanup..."
              VPC_ID=$(aws ec2 describe-vpcs \
                --filters "Name=tag:Project,Values=${{ env.PROJECT_NAME }}" "Name=tag:Environment,Values=${{ env.ENVIRONMENT }}" \
                --query "Vpcs[0].VpcId" --output text 2>/dev/null || echo "None")
              
              if [ "$VPC_ID" != "None" ] && [ "$VPC_ID" != "null" ] && [ ! -z "$VPC_ID" ]; then
                # Force delete any remaining ENIs
                aws ec2 describe-network-interfaces \
                  --filters "Name=vpc-id,Values=$VPC_ID" \
                  --query "NetworkInterfaces[].NetworkInterfaceId" \
                  --output text 2>/dev/null | xargs -n1 -I {} aws ec2 delete-network-interface --network-interface-id {} || true
                
                # Release any remaining EIPs
                aws ec2 describe-addresses \
                  --filters "Name=domain,Values=vpc" \
                  --query "Addresses[].AllocationId" \
                  --output text 2>/dev/null | xargs -n1 -I {} aws ec2 release-address --allocation-id {} || true
              fi
            else
              echo "âŒ All network destruction attempts failed"
              exit 1
            fi
          fi
        done

    - name: Clean Up CloudWatch Resources
      id: cloudwatch_cleanup
      run: |
        echo "ðŸ” Finding CloudWatch resources..."

        # Delete CloudWatch Dashboard
        echo "ðŸ—‘ï¸ Deleting CloudWatch Dashboard..."
        DASHBOARD_NAME="${{ env.PROJECT_NAME }}-${{ env.ENVIRONMENT }}-${{ env.CLUSTER_NAME }}-dashboard"
        if aws cloudwatch describe-dashboards --dashboard-names "$DASHBOARD_NAME" >/dev/null 2>&1; then
          echo "   âœ… Found dashboard: $DASHBOARD_NAME"
          aws cloudwatch delete-dashboards --dashboard-names "$DASHBOARD_NAME" || true
          echo "   âœ… Dashboard deletion completed"
        else
          echo "   â„¹ï¸ Dashboard not found: $DASHBOARD_NAME"
        fi

        # Delete CloudWatch Alarms
        echo "ðŸ—‘ï¸ Deleting CloudWatch Alarms..."

        # First try to find alarms by tags (if they exist)
        TAGGED_ALARMS=$(aws cloudwatch describe-alarms \
          --query "MetricAlarms[?contains(Tags[?Key=='Project'].Value, '${{ env.PROJECT_NAME }}') && contains(Tags[?Key=='Environment'].Value, '${{ env.ENVIRONMENT }}')].AlarmName" \
          --output text 2>/dev/null || echo "")

        if [ ! -z "$TAGGED_ALARMS" ] && [ "$TAGGED_ALARMS" != "None" ]; then
          echo "   âœ… Found tagged alarms: $TAGGED_ALARMS"
          for alarm in $TAGGED_ALARMS; do
            echo "   ðŸ—‘ï¸ Deleting alarm: $alarm"
            aws cloudwatch delete-alarms --alarm-names "$alarm" || true
          done
        fi

        # Also try to find alarms by name pattern (fallback)
        PATTERN_ALARMS=$(aws cloudwatch describe-alarms \
          --query "MetricAlarms[?contains(AlarmName, '${{ env.ENVIRONMENT }}-${{ env.PROJECT_NAME }}-${{ env.CLUSTER_NAME }}')].AlarmName" \
          --output text 2>/dev/null || echo "")

        if [ ! -z "$PATTERN_ALARMS" ] && [ "$PATTERN_ALARMS" != "None" ]; then
          echo "   âœ… Found pattern-matched alarms: $PATTERN_ALARMS"
          for alarm in $PATTERN_ALARMS; do
            echo "   ðŸ—‘ï¸ Deleting alarm: $alarm"
            aws cloudwatch delete-alarms --alarm-names "$alarm" || true
          done
        fi

        if [ -z "$TAGGED_ALARMS" ] && [ -z "$PATTERN_ALARMS" ]; then
          echo "   â„¹ï¸ No CloudWatch alarms found"
        fi

    - name: Clean Up CloudWatch Log Groups
      id: cloudwatch_logs_cleanup
      run: |
        echo "ðŸ” Cleaning up CloudWatch Log Groups..."

        # List of log groups to clean up
        LOG_GROUPS=(
          "/aws/eks/vanillatstodo-cluster/cluster"
          "/aws/vpc/staging-flow-logs"
        )

        # Also check for environment-specific flow logs
        ENV_FLOW_LOGS="/aws/vpc/${{ env.ENVIRONMENT }}-flow-logs"
        LOG_GROUPS+=("$ENV_FLOW_LOGS")

        # Also check for any other log groups with our project name
        PROJECT_LOG_GROUPS=$(aws logs describe-log-groups \
          --query "logGroups[?contains(logGroupName, '${{ env.PROJECT_NAME }}')].logGroupName" \
          --output text 2>/dev/null || echo "")

        if [ ! -z "$PROJECT_LOG_GROUPS" ] && [ "$PROJECT_LOG_GROUPS" != "None" ]; then
          echo "ðŸ” Found project-specific log groups: $PROJECT_LOG_GROUPS"
          for log_group in $PROJECT_LOG_GROUPS; do
            LOG_GROUPS+=("$log_group")
          done
        fi

        # Remove duplicates and clean up each log group
        UNIQUE_LOG_GROUPS=($(printf "%s\n" "${LOG_GROUPS[@]}" | sort -u))

        for log_group in "${UNIQUE_LOG_GROUPS[@]}"; do
          if [ ! -z "$log_group" ]; then
            echo "ðŸ—‘ï¸ Checking log group: $log_group"
            
            # Check if log group exists
            if aws logs describe-log-groups --log-group-name-prefix "$log_group" --query "logGroups[?logGroupName=='$log_group']" --output text | grep -q "$log_group"; then
              echo "   âœ… Log group exists. Deleting..."
              
              # Delete all log streams first
              echo "   ðŸ—‘ï¸ Deleting all log streams..."
              STREAMS=$(aws logs describe-log-streams \
                --log-group-name "$log_group" \
                --query 'logStreams[*].logStreamName' \
                --output text 2>/dev/null || echo "")
              
              if [ ! -z "$STREAMS" ] && [ "$STREAMS" != "None" ]; then
                for stream in $STREAMS; do
                  echo "     ðŸ—‘ï¸ Deleting stream: $stream"
                  aws logs delete-log-stream --log-group-name "$log_group" --log-stream-name "$stream" || true
                done
              fi
              
              # Delete the log group
              echo "   ðŸ—‘ï¸ Deleting log group: $log_group"
              if aws logs delete-log-group --log-group-name "$log_group"; then
                echo "   âœ… Successfully deleted log group: $log_group"
              else
                echo "   âš ï¸ Failed to delete log group: $log_group"
              fi
            else
              echo "   â„¹ï¸ Log group does not exist: $log_group"
            fi
          fi
        done

        echo "âœ… CloudWatch Log Groups cleanup completed"

    - name: Clean Up Network Resources
      id: manual_network_cleanup
      run: |
        # Initialize counters and status tracking
        declare -A CLEANED_COUNT=(
          ["ENDPOINTS"]=0
          ["NAT"]=0
          ["EIP"]=0
          ["INTERFACES"]=0
          ["SECURITY_GROUPS"]=0
          ["ROUTE_TABLES"]=0
          ["NACLS"]=0
          ["SUBNETS"]=0
          ["IGW"]=0
          ["VPC"]=0
        )

        declare -A TOTAL_COUNT=(
          ["ENDPOINTS"]=0
          ["NAT"]=0
          ["EIP"]=0
          ["INTERFACES"]=0
          ["SECURITY_GROUPS"]=0
          ["ROUTE_TABLES"]=0
          ["NACLS"]=0
          ["SUBNETS"]=0
          ["IGW"]=0
          ["VPC"]=0
        )

        # Clean up EIPs with environment-project-nat pattern
        echo "ðŸ” Finding EIPs with ${{ env.ENVIRONMENT }}-${{ env.PROJECT_NAME }}-nat pattern..."
        STAGING_EIPS=$(aws ec2 describe-addresses \
          --filters "Name=tag:Name,Values=${{ env.ENVIRONMENT }}-${{ env.PROJECT_NAME }}-nat-*" \
          --query 'Addresses[*].AllocationId' \
          --output text || echo "")

        if [ ! -z "$STAGING_EIPS" ]; then
          for eip in $STAGING_EIPS; do
            if [ ! -z "$eip" ]; then
              ((TOTAL_COUNT["EIP"]++))
              EIP_NAME=$(aws ec2 describe-addresses \
                --allocation-ids $eip \
                --query 'Addresses[0].Tags[?Key==`Name`].Value' \
                --output text)
              echo "   Releasing EIP: $EIP_NAME ($eip)"
              if aws ec2 release-address --allocation-id $eip; then
                ((CLEANED_COUNT["EIP"]++))
                echo "   âœ… Successfully released EIP: $EIP_NAME"
              else
                echo "   âš ï¸ Failed to release EIP: $EIP_NAME"
              fi
              sleep 2
            fi
          done
        else
          echo "   No ${{ env.ENVIRONMENT }} NAT EIPs found"
        fi

        echo "ðŸ” Finding all VPCs with vanillatstodo tag..."
        VPC_IDS=$(aws ec2 describe-vpcs \
          --filters "Name=tag:Name,Values=*vanillatstodo*" \
          --query 'Vpcs[*].VpcId' \
          --output text)

        for VPC_ID in $VPC_IDS; do
          ((TOTAL_COUNT["VPC"]++))
          echo "ðŸ”„ Processing VPC: $VPC_ID"

          # 1. VPC Endpoints
          echo "ðŸ—‘ï¸ Cleaning up VPC Endpoints..."
          ENDPOINTS=$(aws ec2 describe-vpc-endpoints \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query 'VpcEndpoints[*].VpcEndpointId' \
            --output text)
          
          for endpoint in $ENDPOINTS; do
            ((TOTAL_COUNT["ENDPOINTS"]++))
            echo "   Deleting endpoint: $endpoint"
            if aws ec2 delete-vpc-endpoints --vpc-endpoint-ids $endpoint; then
              ((CLEANED_COUNT["ENDPOINTS"]++))
            fi
            sleep 5
          done

          # 2. NAT Gateways
          echo "ðŸ—‘ï¸ Cleaning up NAT Gateways..."
          NAT_IDS=$(aws ec2 describe-nat-gateways \
            --filter "Name=vpc-id,Values=$VPC_ID" \
            --query 'NatGateways[?State!=`deleted`].NatGatewayId' \
            --output text)
          
          for nat in $NAT_IDS; do
            ((TOTAL_COUNT["NAT"]++))
            echo "   Deleting NAT Gateway: $nat"
            if aws ec2 delete-nat-gateway --nat-gateway-id $nat; then
              ((CLEANED_COUNT["NAT"]++))
            fi
          done

          [ ! -z "$NAT_IDS" ] && sleep 45

          # 3. Network Interfaces
          echo "ðŸ—‘ï¸ Cleaning up Network Interfaces..."
          ENI_IDS=$(aws ec2 describe-network-interfaces \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query 'NetworkInterfaces[*].NetworkInterfaceId' \
            --output text)
          
          for eni in $ENI_IDS; do
            ((TOTAL_COUNT["INTERFACES"]++))
            # Check if ENI still exists before checking status
            if aws ec2 describe-network-interfaces --network-interface-ids $eni >/dev/null 2>&1; then
              ENI_STATUS=$(aws ec2 describe-network-interfaces \
                --network-interface-ids $eni \
                --query 'NetworkInterfaces[0].Status' \
                --output text 2>/dev/null || echo "not-found")
              
              if [ "$ENI_STATUS" = "available" ]; then
                echo "   Deleting Network Interface: $eni"
                if aws ec2 delete-network-interface --network-interface-id $eni; then
                  ((CLEANED_COUNT["INTERFACES"]++))
                fi
              elif [ "$ENI_STATUS" = "not-found" ]; then
                echo "   â„¹ï¸ Network Interface $eni already deleted"
                ((CLEANED_COUNT["INTERFACES"]++))
              else
                echo "   âš ï¸ Network Interface $eni is not available (status: $ENI_STATUS)"
              fi
            else
              echo "   â„¹ï¸ Network Interface $eni no longer exists (already cleaned up)"
              ((CLEANED_COUNT["INTERFACES"]++))
            fi
            sleep 2
          done

          # 4. Security Groups
          echo "ðŸ—‘ï¸ Cleaning up Security Groups..."
          SG_IDS=$(aws ec2 describe-security-groups \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query 'SecurityGroups[?GroupName!=`default`].GroupId' \
            --output text)
          
          for sg in $SG_IDS; do
            ((TOTAL_COUNT["SECURITY_GROUPS"]++))
            echo "   Deleting Security Group: $sg"
            if aws ec2 delete-security-group --group-id $sg; then
              ((CLEANED_COUNT["SECURITY_GROUPS"]++))
            fi
            sleep 2
          done

          # 5. Route Tables
          echo "ðŸ—‘ï¸ Cleaning up Route Tables..."
          RT_IDS=$(aws ec2 describe-route-tables \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query 'RouteTables[?Associations[0].Main!=`true`].RouteTableId' \
            --output text)
          
          for rt in $RT_IDS; do
            ((TOTAL_COUNT["ROUTE_TABLES"]++))
            ASSOC_IDS=$(aws ec2 describe-route-tables \
              --route-table-id $rt \
              --query 'RouteTables[0].Associations[*].RouteTableAssociationId' \
              --output text)
            
            for assoc in $ASSOC_IDS; do
              aws ec2 disassociate-route-table --association-id $assoc
            done
            
            if aws ec2 delete-route-table --route-table-id $rt; then
              ((CLEANED_COUNT["ROUTE_TABLES"]++))
            fi
            sleep 2
          done

          # 6. Network ACLs
          echo "ðŸ—‘ï¸ Cleaning up Network ACLs..."
          NACL_IDS=$(aws ec2 describe-network-acls \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query 'NetworkAcls[?!IsDefault].NetworkAclId' \
            --output text)
          
          for nacl in $NACL_IDS; do
            ((TOTAL_COUNT["NACLS"]++))
            if aws ec2 delete-network-acl --network-acl-id $nacl; then
              ((CLEANED_COUNT["NACLS"]++))
            fi
            sleep 2
          done

          # 7. Subnets
          echo "ðŸ—‘ï¸ Cleaning up Subnets..."
          SUBNET_IDS=$(aws ec2 describe-subnets \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query 'Subnets[*].SubnetId' \
            --output text)
          
          for subnet in $SUBNET_IDS; do
            ((TOTAL_COUNT["SUBNETS"]++))
            if aws ec2 delete-subnet --subnet-id $subnet; then
              ((CLEANED_COUNT["SUBNETS"]++))
            fi
            sleep 2
          done

          # 8. Internet Gateway
          echo "ðŸ—‘ï¸ Cleaning up Internet Gateway..."
          IGW_ID=$(aws ec2 describe-internet-gateways \
            --filters "Name=attachment.vpc-id,Values=$VPC_ID" \
            --query 'InternetGateways[*].InternetGatewayId' \
            --output text)
          
          if [ ! -z "$IGW_ID" ] && [ "$IGW_ID" != "None" ]; then
            ((TOTAL_COUNT["IGW"]++))
            if aws ec2 detach-internet-gateway --internet-gateway-id $IGW_ID --vpc-id $VPC_ID && \
               aws ec2 delete-internet-gateway --internet-gateway-id $IGW_ID; then
              ((CLEANED_COUNT["IGW"]++))
            fi
            sleep 2
          fi

          # 9. VPC
          echo "ðŸ—‘ï¸ Deleting VPC: $VPC_ID"
          if aws ec2 delete-vpc --vpc-id $VPC_ID; then
            ((CLEANED_COUNT["VPC"]++))
          fi
        done

        # Update GitHub Environment variables with counts and status
        {
          for resource in "${!CLEANED_COUNT[@]}"; do
            echo "${resource}_CLEANED=${CLEANED_COUNT[$resource]}" >> $GITHUB_ENV
            echo "${resource}_TOTAL=${TOTAL_COUNT[$resource]}" >> $GITHUB_ENV
            if [ "${CLEANED_COUNT[$resource]}" -eq "${TOTAL_COUNT[$resource]}" ]; then
              echo "NETWORK_${resource}_STATUS=âœ…" >> $GITHUB_ENV
            else
              echo "NETWORK_${resource}_STATUS=âŒ" >> $GITHUB_ENV
            fi
          done

          # Set overall network status
          if [ "${CLEANED_COUNT[VPC]}" -eq "${TOTAL_COUNT[VPC]}" ] && \
             [ "${CLEANED_COUNT[EIP]}" -eq "${TOTAL_COUNT[EIP]}" ]; then
            echo "NETWORK_STATUS=âœ…" >> $GITHUB_ENV
          else
            echo "NETWORK_STATUS=âŒ" >> $GITHUB_ENV
          fi
        }

    - name: Skip IAM Resources Destruction (Manual OIDC Role)
      id: iam_cleanup
      run: |
        echo "â„¹ï¸ Skipping IAM destruction - using manually created OIDC role"
        echo "âš ï¸ Manual IAM role and OIDC provider must be deleted manually if needed"
        echo "Role ARN: ${{ secrets.AWS_ROLE_ARN }}"

    - name: Clean Up S3 Resources
      id: s3_cleanup
      run: |
        echo "ðŸ” Finding S3 resources..."

        # List and delete S3 buckets with our project name pattern
        BUCKETS=$(aws s3api list-buckets --query "Buckets[?contains(Name, '${{ env.PROJECT_NAME }}')].Name" --output text)
        if [ ! -z "$BUCKETS" ] && [ "$BUCKETS" != "None" ]; then
          for bucket in $BUCKETS; do
            echo "ðŸ—‘ï¸ Processing S3 bucket: $bucket"
            
            # Check if bucket exists and is accessible
            if aws s3api head-bucket --bucket $bucket 2>/dev/null; then
              echo "   âœ… Bucket exists and is accessible"
              
              # 1. Check if versioning is enabled
              VERSIONING=$(aws s3api get-bucket-versioning --bucket $bucket --query 'Status' --output text 2>/dev/null || echo "None")
              echo "   ðŸ“Š Versioning status: $VERSIONING"
              
              if [ "$VERSIONING" = "Enabled" ]; then
                echo "   ðŸ”„ Versioning is enabled. Removing all versions and delete markers..."
                
                # Remove all versions and delete markers
                aws s3api list-object-versions \
                  --bucket $bucket \
                  --query '{Objects: Versions[].{Key:Key,VersionId:VersionId}}' \
                  --output json > /tmp/versions.json
                
                if [ -s /tmp/versions.json ] && [ "$(jq -r '.Objects | length' /tmp/versions.json)" -gt 0 ]; then
                  echo "   ðŸ—‘ï¸ Deleting all object versions..."
                  aws s3api delete-objects --bucket $bucket --delete file:///tmp/versions.json || true
                fi
                
                # Remove all delete markers
                aws s3api list-object-versions \
                  --bucket $bucket \
                  --query '{Objects: DeleteMarkers[].{Key:Key,VersionId:VersionId}}' \
                  --output json > /tmp/delete-markers.json
                
                if [ -s /tmp/delete-markers.json ] && [ "$(jq -r '.Objects | length' /tmp/delete-markers.json)" -gt 0 ]; then
                  echo "   ðŸ—‘ï¸ Deleting all delete markers..."
                  aws s3api delete-objects --bucket $bucket --delete file:///tmp/delete-markers.json || true
                fi
              fi
              
              # 2. Remove all objects (current versions)
              echo "   ðŸ—‘ï¸ Removing all current objects..."
              aws s3 rm s3://$bucket --recursive || true
              
              # 3. Check for any remaining objects
              REMAINING_OBJECTS=$(aws s3api list-objects-v2 --bucket $bucket --query 'Contents[].Key' --output text 2>/dev/null || echo "")
              if [ ! -z "$REMAINING_OBJECTS" ] && [ "$REMAINING_OBJECTS" != "None" ]; then
                echo "   âš ï¸ Found remaining objects: $REMAINING_OBJECTS"
                echo "   ðŸ—‘ï¸ Force removing remaining objects..."
                aws s3 rm s3://$bucket --recursive --force || true
              fi
              
              # 4. Check for any remaining versions
              REMAINING_VERSIONS=$(aws s3api list-object-versions --bucket $bucket --query 'Versions[].Key' --output text 2>/dev/null || echo "")
              if [ ! -z "$REMAINING_VERSIONS" ] && [ "$REMAINING_VERSIONS" != "None" ]; then
                echo "   âš ï¸ Found remaining versions: $REMAINING_VERSIONS"
                echo "   ðŸ—‘ï¸ Force removing remaining versions..."
                aws s3api list-object-versions \
                  --bucket $bucket \
                  --query '{Objects: Versions[].{Key:Key,VersionId:VersionId}}' \
                  --output json | aws s3api delete-objects --bucket $bucket --delete file:///dev/stdin || true
              fi
              
              # 5. Final verification - check if bucket is truly empty
              FINAL_CHECK=$(aws s3api list-objects-v2 --bucket $bucket --query 'Contents[].Key' --output text 2>/dev/null || echo "")
              FINAL_VERSIONS=$(aws s3api list-object-versions --bucket $bucket --query 'Versions[].Key' --output text 2>/dev/null || echo "")
              
              if [ -z "$FINAL_CHECK" ] && [ -z "$FINAL_VERSIONS" ]; then
                echo "   âœ… Bucket is empty. Proceeding with deletion..."
                aws s3api delete-bucket --bucket $bucket || true
                echo "   âœ… Bucket deletion completed"
              else
                echo "   âš ï¸ Bucket still has content. Attempting force deletion..."
                aws s3 rb s3://$bucket --force || true
              fi
            else
              echo "   âš ï¸ Bucket does not exist or is not accessible"
            fi
            
            echo "   ---"
          done
        else
          echo "No S3 buckets found with project name pattern"
        fi

    - name: Clean Up VPC Resources
      id: vpc_cleanup
      run: |
        echo "ðŸ” Finding VPC resources..."

        # List and delete VPCs with our tags
        VPCS=$(aws ec2 describe-vpcs --filters "Name=tag:Project,Values=${{ env.PROJECT_NAME }}" "Name=tag:Environment,Values=${{ env.ENVIRONMENT }}" --query "Vpcs[].VpcId" --output text)
        if [ ! -z "$VPCS" ]; then
          for vpc in $VPCS; do
            echo "ðŸ—‘ï¸ Deleting VPC: $vpc"
            aws ec2 delete-vpc --vpc-id $vpc
          done
        fi

    - name: Summarize Cleanup Status
      if: always()
      run: |
        echo "### Cleanup Summary ðŸ§¹" >> $GITHUB_STEP_SUMMARY
        echo "| Resource | Status | Details |" >> $GITHUB_STEP_SUMMARY
        echo "| -------- | ------ | ------- |" >> $GITHUB_STEP_SUMMARY
        echo "| EKS Cluster | ${EKS_STATUS:-âŒ} | Version: 1.31 |" >> $GITHUB_STEP_SUMMARY
        echo "| K8s Resources | ${K8S_CLEANUP_STATUS:-âŒ} | LoadBalancers, PVCs, Ingresses |" >> $GITHUB_STEP_SUMMARY

        # Monitoring status based on whether Terraform failed
        if [ "${MONITORING_TERRAFORM_FAILED:-false}" = "true" ]; then
          echo "| Monitoring | âš ï¸ | Terraform failed, manual cleanup performed |" >> $GITHUB_STEP_SUMMARY
        else
          echo "| Monitoring | ${MONITORING_STATUS:-âœ…} | CloudWatch resources |" >> $GITHUB_STEP_SUMMARY
        fi

        echo "| CloudWatch | ${CLOUDWATCH_STATUS:-âŒ} | Dashboards & Alarms |" >> $GITHUB_STEP_SUMMARY

        # Network resources with cleanup details - using correct variable names
        echo "| Network - EIP | ${NETWORK_EIP_STATUS:-âŒ} | Cleaned: ${EIP_CLEANED:-0} of ${EIP_TOTAL:-0} |" >> $GITHUB_STEP_SUMMARY
        echo "| Network - VPC Endpoints | ${NETWORK_ENDPOINTS_STATUS:-âŒ} | Cleaned: ${ENDPOINTS_CLEANED:-0} of ${ENDPOINTS_TOTAL:-0} |" >> $GITHUB_STEP_SUMMARY
        echo "| Network - NAT Gateways | ${NETWORK_NAT_STATUS:-âŒ} | Cleaned: ${NAT_CLEANED:-0} of ${NAT_TOTAL:-0} |" >> $GITHUB_STEP_SUMMARY
        echo "| Network - Route Tables | ${NETWORK_ROUTE_TABLES_STATUS:-âŒ} | Cleaned: ${ROUTE_TABLES_CLEANED:-0} of ${ROUTE_TABLES_TOTAL:-0} |" >> $GITHUB_STEP_SUMMARY
        echo "| Network - NACLs | ${NETWORK_NACLS_STATUS:-âŒ} | Cleaned: ${NACLS_CLEANED:-0} of ${NACLS_TOTAL:-0} |" >> $GITHUB_STEP_SUMMARY
        echo "| Network - Subnets | ${NETWORK_SUBNETS_STATUS:-âŒ} | Cleaned: ${SUBNETS_CLEANED:-0} of ${SUBNETS_TOTAL:-0} |" >> $GITHUB_STEP_SUMMARY
        echo "| Network - IGW | ${NETWORK_IGW_STATUS:-âŒ} | Cleaned: ${IGW_CLEANED:-0} of ${IGW_TOTAL:-0} |" >> $GITHUB_STEP_SUMMARY
        echo "| Network - VPCs | ${NETWORK_VPC_STATUS:-âŒ} | Cleaned: ${VPC_CLEANED:-0} of ${VPC_TOTAL:-0} |" >> $GITHUB_STEP_SUMMARY
        echo "| Network - Overall | ${NETWORK_STATUS:-âŒ} | Network Stack |" >> $GITHUB_STEP_SUMMARY

        echo "| S3 State | ${S3_STATUS:-âŒ} | Terraform State |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Configuration:**" >> $GITHUB_STEP_SUMMARY
        echo "- Terraform Version: \`${TF_VERSION}\`" >> $GITHUB_STEP_SUMMARY
        echo "- Region: \`${AWS_REGION}\`" >> $GITHUB_STEP_SUMMARY
        echo "- Project: \`${PROJECT_NAME}\`" >> $GITHUB_STEP_SUMMARY
        echo "- Timestamp: \`$(date -u '+%Y-%m-%d %H:%M:%S UTC')\`" >> $GITHUB_STEP_SUMMARY
