name: Destroy CodeBase on AWS

on:
  workflow_dispatch:
    inputs:
      cleanup_type:
        description: "Type of cleanup to perform"
        required: true
        default: "deployment"
        type: choice
        options:
          - deployment # Just the app deployment
          - all-k8s # All Kubernetes resources
          - failed-pods # Only failed/stuck pods
      environment:
        description: "Environment to cleanup"
        required: true
        default: "experimental"
        type: choice
        options:
          - staging
          - production
          - experimental
      confirm:
        description: "Type 'cleanup' to confirm"
        required: true
        type: string

permissions:
  id-token: write
  contents: read

env:
  CLUSTER_NAME: "vanillatstodo-cluster"
  AWS_REGION: "us-east-2"
  APP_NAME: "vanillatstodo"
  ENVIRONMENT: ${{ github.event.inputs.environment }}

jobs:
  cleanup:
    runs-on: ubuntu-latest
    if: github.event.inputs.confirm == 'cleanup'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Load Environment Variables
        run: |
          cat .github/variables/environment.env >> $GITHUB_ENV

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

      - name: Verify EKS Cluster
        run: |
          echo "ðŸ” Verifying EKS cluster..."
          if ! aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} >/dev/null 2>&1; then
            echo "âŒ EKS cluster not found!"
            exit 1
          fi
          echo "âœ… EKS cluster verified"

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig \
            --name ${{ env.CLUSTER_NAME }} \
            --region ${{ env.AWS_REGION }}

      - name: Pre-Cleanup Status
        run: |
          echo "ðŸ“Š Current Kubernetes Status:"
          echo "================================"

          echo "ðŸ” Deployments:"
          kubectl get deployments -l app=${{ env.APP_NAME }} -o wide || echo "No deployments found"

          echo ""
          echo "ðŸ” Pods:"
          kubectl get pods -l app=${{ env.APP_NAME }} -o wide || echo "No pods found"

          echo ""
          echo "ðŸ” Services:"
          kubectl get services -l app=${{ env.APP_NAME }} -o wide || echo "No services found"

          echo ""
          echo "ðŸ” ReplicaSets:"
          kubectl get replicasets -l app=${{ env.APP_NAME }} -o wide || echo "No replicasets found"

      - name: Cleanup Deployment
        if: github.event.inputs.cleanup_type == 'deployment' || github.event.inputs.cleanup_type == 'all-k8s'
        run: |
          echo "ðŸ—‘ï¸ Cleaning up deployment..."

          # Delete deployment
          if kubectl get deployment ${{ env.APP_NAME }} >/dev/null 2>&1; then
            echo "âœ… Found deployment ${{ env.APP_NAME }}, deleting..."
            kubectl delete deployment ${{ env.APP_NAME }} --timeout=60s
            echo "âœ… Deployment deleted"
          else
            echo "â„¹ï¸ No deployment found"
          fi

          # Wait for pods to terminate
          echo "â³ Waiting for pods to terminate..."
          kubectl wait --for=delete pods -l app=${{ env.APP_NAME }} --timeout=120s || true

      - name: Cleanup Failed Pods
        if: github.event.inputs.cleanup_type == 'failed-pods' || github.event.inputs.cleanup_type == 'all-k8s'
        run: |
          echo "ðŸ—‘ï¸ Cleaning up failed/stuck pods..."

          # Delete failed pods
          FAILED_PODS=$(kubectl get pods -l app=${{ env.APP_NAME }} --field-selector=status.phase!=Running,status.phase!=Succeeded -o name 2>/dev/null || echo "")

          if [ ! -z "$FAILED_PODS" ]; then
            echo "âœ… Found failed pods, deleting..."
            echo "$FAILED_PODS"
            kubectl delete $FAILED_PODS --force --grace-period=0 || true
          else
            echo "â„¹ï¸ No failed pods found"
          fi

          # Delete pending pods
          PENDING_PODS=$(kubectl get pods -l app=${{ env.APP_NAME }} --field-selector=status.phase=Pending -o name 2>/dev/null || echo "")

          if [ ! -z "$PENDING_PODS" ]; then
            echo "âœ… Found pending pods, deleting..."
            echo "$PENDING_PODS"
            kubectl delete $PENDING_PODS --force --grace-period=0 || true
          else
            echo "â„¹ï¸ No pending pods found"
          fi

      - name: Cleanup ReplicaSets
        if: github.event.inputs.cleanup_type == 'all-k8s'
        run: |
          echo "ðŸ—‘ï¸ Cleaning up old ReplicaSets..."

          # Delete replicasets
          OLD_RS=$(kubectl get replicasets -l app=${{ env.APP_NAME }} -o name 2>/dev/null || echo "")

          if [ ! -z "$OLD_RS" ]; then
            echo "âœ… Found old ReplicaSets, deleting..."
            echo "$OLD_RS"
            kubectl delete $OLD_RS || true
          else
            echo "â„¹ï¸ No ReplicaSets found"
          fi

      - name: Cleanup Services and LoadBalancers
        if: github.event.inputs.cleanup_type == 'deployment' || github.event.inputs.cleanup_type == 'all-k8s'
        run: |
          echo "ðŸ—‘ï¸ Cleaning up services and AWS LoadBalancers..."

          # Find and delete LoadBalancer services (these create AWS LoadBalancers)
          LB_SERVICES=$(kubectl get services -l app=${{ env.APP_NAME }} --field-selector spec.type=LoadBalancer -o name 2>/dev/null || echo "")

          if [ ! -z "$LB_SERVICES" ]; then
            echo "âœ… Found LoadBalancer services: $LB_SERVICES"
            echo "ðŸ—‘ï¸ Deleting LoadBalancer services (this will also delete AWS LoadBalancers)..."
            kubectl delete $LB_SERVICES --timeout=180s || true
            
            echo "â³ Waiting for AWS LoadBalancer deletion to propagate..."
            sleep 60
            
            # Verify AWS LoadBalancer is being deleted
            echo "ðŸ” Checking AWS LoadBalancer deletion status..."
            aws elbv2 describe-load-balancers --query "LoadBalancers[?contains(LoadBalancerName, '${{ env.APP_NAME }}') || contains(LoadBalancerName, 'a4a3c58a')].{Name:LoadBalancerName,State:State.Code}" --output table || echo "No LoadBalancers found (good!)"
          else
            echo "â„¹ï¸ No LoadBalancer services found"
          fi

          # Clean up any remaining services for the app
          REMAINING_SERVICES=$(kubectl get services -l app=${{ env.APP_NAME }} -o name 2>/dev/null || echo "")

          if [ ! -z "$REMAINING_SERVICES" ]; then
            echo "âœ… Found remaining services: $REMAINING_SERVICES"
            echo "ðŸ—‘ï¸ Deleting remaining services..."
            kubectl delete $REMAINING_SERVICES --timeout=60s || true
          else
            echo "â„¹ï¸ No remaining services found"
          fi

      - name: Force Cleanup Stuck Resources
        if: github.event.inputs.cleanup_type == 'all-k8s'
        run: |
          echo "ðŸ”¥ Force cleaning any stuck resources..."

          # Force delete any remaining pods
          kubectl delete pods -l app=${{ env.APP_NAME }} --force --grace-period=0 || true

          # Remove any finalizers that might be blocking deletion
          kubectl patch deployment ${{ env.APP_NAME }} -p '{"metadata":{"finalizers":[]}}' --type=merge || true

      - name: Post-Cleanup Status
        run: |
          echo "ðŸ“Š Post-Cleanup Kubernetes Status:"
          echo "==================================="

          echo "ðŸ” Deployments:"
          kubectl get deployments -l app=${{ env.APP_NAME }} -o wide || echo "âœ… No deployments found"

          echo ""
          echo "ðŸ” Pods:"
          kubectl get pods -l app=${{ env.APP_NAME }} -o wide || echo "âœ… No pods found"

          echo ""
          echo "ðŸ” Services:"
          kubectl get services -l app=${{ env.APP_NAME }} -o wide || echo "âœ… No services found"

          echo ""
          echo "ðŸ” ReplicaSets:"
          kubectl get replicasets -l app=${{ env.APP_NAME }} -o wide || echo "âœ… No replicasets found"

          echo ""
          echo "ðŸ” AWS LoadBalancer Status:"
          aws elbv2 describe-load-balancers --query "LoadBalancers[?contains(LoadBalancerName, '${{ env.APP_NAME }}') || contains(LoadBalancerName, 'a4a3c58a')].{Name:LoadBalancerName,State:State.Code}" --output table || echo "âœ… No LoadBalancers found"

      - name: Final LoadBalancer Cleanup Verification
        run: |
          echo "ðŸ” Final verification - checking for any remaining LoadBalancers..."

          # Wait a bit more for AWS propagation
          sleep 30

          # Check for any LoadBalancers that might still be deleting
          REMAINING_LBS=$(aws elbv2 describe-load-balancers --query "LoadBalancers[?contains(LoadBalancerName, '${{ env.APP_NAME }}') || contains(LoadBalancerName, 'a4a3c58a')].LoadBalancerName" --output text 2>/dev/null || echo "")

          if [ ! -z "$REMAINING_LBS" ] && [ "$REMAINING_LBS" != "None" ]; then
            echo "âš ï¸ Warning: Some LoadBalancers may still be in deletion process:"
            echo "$REMAINING_LBS"
            echo "â„¹ï¸ This is normal - AWS LoadBalancer deletion can take 2-5 minutes"
            echo "â„¹ï¸ Verify manually if infrastructure destruction fails"
          else
            echo "âœ… All LoadBalancers successfully cleaned up"
          fi

      - name: Cleanup Summary
        if: always()
        run: |
          echo "### Cleanup Summary ðŸ§¹" >> $GITHUB_STEP_SUMMARY
          echo "| Resource | Action | Status |" >> $GITHUB_STEP_SUMMARY
          echo "| -------- | ------ | ------ |" >> $GITHUB_STEP_SUMMARY
          echo "| Cleanup Type | ${{ github.event.inputs.cleanup_type }} | âœ… |" >> $GITHUB_STEP_SUMMARY
          echo "| Environment | ${{ env.ENVIRONMENT }} | âœ… |" >> $GITHUB_STEP_SUMMARY
          echo "| Cluster | ${{ env.CLUSTER_NAME }} | âœ… |" >> $GITHUB_STEP_SUMMARY
          echo "| Region | ${{ env.AWS_REGION }} | âœ… |" >> $GITHUB_STEP_SUMMARY
          echo "| LoadBalancer Cleanup | âœ… | AWS ELB/NLB deleted |" >> $GITHUB_STEP_SUMMARY
          echo "| Status | ${{ job.status }} | Job Status |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Cleanup completed!** LoadBalancers have been properly cleaned up." >> $GITHUB_STEP_SUMMARY
          echo "You can now safely run infrastructure destruction without dependency issues." >> $GITHUB_STEP_SUMMARY
