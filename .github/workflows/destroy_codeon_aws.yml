name: Destroy CodeBase on AWS

on:
  workflow_dispatch:
    inputs:
      cleanup_type:
        description: "Type of cleanup to perform"
        required: true
        default: "deployment"
        type: choice
        options:
          - deployment # Just the app deployment
          - all-k8s # All Kubernetes resources
          - failed-pods # Only failed/stuck pods
      environment:
        description: "Environment to cleanup"
        required: true
        default: "experimental"
        type: choice
        options:
          - staging
          - production
          - experimental
      confirm:
        description: "Type 'cleanup' to confirm"
        required: true
        type: string

permissions:
  id-token: write
  contents: read

env:
  CLUSTER_NAME: ${{ vars.CLUSTER_NAME || 'vanillatstodo-cluster' }}
  AWS_REGION: ${{ vars.AWS_REGION || 'us-east-2' }}
  APP_NAME: ${{ vars.APP_NAME || 'vanillatstodo' }}
  ENVIRONMENT: ${{ github.event.inputs.environment }}

jobs:
  cleanup:
    runs-on: ubuntu-latest
    if: github.event.inputs.confirm == 'cleanup'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Get IAM Role ARN
        id: iam_role_arn
        run: |
          # Try to get role ARN from IAM Terraform state
          cd devops/terraform/04_iam
          terraform init -reconfigure \
            -backend-config="bucket=${{ vars.BUCKET_NAME || 'vanillatstodo-terraform-state' }}" \
            -backend-config="key=${{ env.ENVIRONMENT }}/iam/terraform.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}" \
            -backend-config="encrypt=true" >/dev/null 2>&1 || true
          
          # Check if IAM resources exist and get role ARN
          if terraform output -raw github_actions_role_arn 2>/dev/null; then
            ROLE_ARN=$(terraform output -raw github_actions_role_arn)
            echo "github_actions_role_arn=${ROLE_ARN}" >> $GITHUB_OUTPUT
            echo "âœ… Found IAM role ARN: ${ROLE_ARN}"
          else
            # Fallback to secret if IAM state not available
            ROLE_ARN="${{ secrets.AWS_ROLE_ARN }}"
            echo "github_actions_role_arn=${ROLE_ARN}" >> $GITHUB_OUTPUT
            echo "âš ï¸ Using fallback role ARN from secrets"
          fi

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ steps.iam_role_arn.outputs.github_actions_role_arn }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/

      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: "3.12.3"

      - name: Verify EKS Cluster
        run: |
          echo "ðŸ” Verifying EKS cluster..."
          if ! aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} >/dev/null 2>&1; then
            echo "âŒ EKS cluster not found!"
            exit 1
          fi
          echo "âœ… EKS cluster verified"

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig \
            --name ${{ env.CLUSTER_NAME }} \
            --region ${{ env.AWS_REGION }} \
            --role-arn ${{ steps.iam_role_arn.outputs.github_actions_role_arn }}

      - name: Pre-Cleanup Status
        run: |
          echo "ðŸ“Š Current Kubernetes Status:"
          echo "================================"

          echo "ðŸ” Helm Releases:"
          echo "ðŸ” Checking Helm releases..."
          helm list -A | grep ${{ env.APP_NAME }} || echo "No Helm releases found"

          echo ""
          echo "ðŸ” Deployments:"
          kubectl get deployments -l app=${{ env.APP_NAME }} -o wide || echo "No deployments found"

          echo ""
          echo "ðŸ” Pods:"
          kubectl get pods -l app=${{ env.APP_NAME }} -o wide || echo "No pods found"

          echo ""
          echo "ðŸ” Services:"
          kubectl get services -l app=${{ env.APP_NAME }} -o wide || echo "No services found"

      - name: Cleanup Helm Release
        if: github.event.inputs.cleanup_type == 'deployment' || github.event.inputs.cleanup_type == 'all-k8s'
        run: |
          echo "ðŸ—‘ï¸ Cleaning up Helm release..."

          run: |
          echo "ðŸ—‘ï¸ Uninstalling Helm releases..."

          # List all app releases
          RELEASES=$(helm list -A -o json | jq -r '.[] | select(.name | contains("${{ env.APP_NAME }}")) | .name' 2>/dev/null || echo "")

          if [ ! -z "$RELEASES" ]; then
            echo "âœ… Found Helm releases:"
            echo "$RELEASES"
            
            # Uninstall each release
            echo "$RELEASES" | while read -r release; do
              echo "ðŸ—‘ï¸ Uninstalling Helm release: $release"
              helm uninstall "$release" --timeout=300s || true
            done
            
            echo "â³ Waiting for resources to be cleaned up..."
            sleep 30
          else
            echo "â„¹ï¸ No Helm releases found"
          fi

          # Also clean up any resources that might remain
          if kubectl get deployment ${{ env.APP_NAME }} >/dev/null 2>&1; then
            echo "âš ï¸ Found remaining deployment, cleaning up manually..."
            kubectl delete deployment ${{ env.APP_NAME }} --timeout=60s || true
          fi

      - name: Cleanup Failed Pods
        if: github.event.inputs.cleanup_type == 'failed-pods' || github.event.inputs.cleanup_type == 'all-k8s'
        run: |
          echo "ðŸ—‘ï¸ Cleaning up failed/stuck pods..."

          # Delete failed pods
          FAILED_PODS=$(kubectl get pods -l app=${{ env.APP_NAME }} --field-selector=status.phase!=Running,status.phase!=Succeeded -o name 2>/dev/null || echo "")

          if [ ! -z "$FAILED_PODS" ]; then
            echo "âœ… Found failed pods, deleting..."
            echo "$FAILED_PODS"
            kubectl delete $FAILED_PODS --force --grace-period=0 || true
          else
            echo "â„¹ï¸ No failed pods found"
          fi

          # Delete pending pods
          PENDING_PODS=$(kubectl get pods -l app=${{ env.APP_NAME }} --field-selector=status.phase=Pending -o name 2>/dev/null || echo "")

          if [ ! -z "$PENDING_PODS" ]; then
            echo "âœ… Found pending pods, deleting..."
            echo "$PENDING_PODS"
            kubectl delete $PENDING_PODS --force --grace-period=0 || true
          else
            echo "â„¹ï¸ No pending pods found"
          fi

      - name: Cleanup ReplicaSets
        if: github.event.inputs.cleanup_type == 'all-k8s'
        run: |
          echo "ðŸ—‘ï¸ Cleaning up old ReplicaSets..."

          # Delete replicasets
          OLD_RS=$(kubectl get replicasets -l app=${{ env.APP_NAME }} -o name 2>/dev/null || echo "")

          if [ ! -z "$OLD_RS" ]; then
            echo "âœ… Found old ReplicaSets, deleting..."
            echo "$OLD_RS"
            kubectl delete $OLD_RS || true
          else
            echo "â„¹ï¸ No ReplicaSets found"
          fi

      - name: Cleanup Services and LoadBalancers
        if: github.event.inputs.cleanup_type == 'deployment' || github.event.inputs.cleanup_type == 'all-k8s'
        run: |
          echo "ðŸ—‘ï¸ Cleaning up services and AWS LoadBalancers..."

          # Find and delete LoadBalancer services (these create AWS LoadBalancers)
          LB_SERVICES=$(kubectl get services -l app=${{ env.APP_NAME }} --field-selector spec.type=LoadBalancer -o name 2>/dev/null || echo "")

          if [ ! -z "$LB_SERVICES" ]; then
            echo "âœ… Found LoadBalancer services: $LB_SERVICES"
            echo "ðŸ—‘ï¸ Deleting LoadBalancer services (this will also delete AWS LoadBalancers)..."
            kubectl delete $LB_SERVICES --timeout=180s || true
            
            echo "â³ Waiting for AWS LoadBalancer deletion to propagate..."
            sleep 60
            
            # Verify AWS LoadBalancer is being deleted
            echo "ðŸ” Checking AWS LoadBalancer deletion status..."
            aws elbv2 describe-load-balancers --query "LoadBalancers[?contains(LoadBalancerName, '${{ env.APP_NAME }}') || contains(LoadBalancerName, 'a4a3c58a')].{Name:LoadBalancerName,State:State.Code}" --output table || echo "No LoadBalancers found (good!)"
          else
            echo "â„¹ï¸ No LoadBalancer services found"
          fi

          # Clean up any remaining services for the app
          REMAINING_SERVICES=$(kubectl get services -l app=${{ env.APP_NAME }} -o name 2>/dev/null || echo "")

          if [ ! -z "$REMAINING_SERVICES" ]; then
            echo "âœ… Found remaining services: $REMAINING_SERVICES"
            echo "ðŸ—‘ï¸ Deleting remaining services..."
            kubectl delete $REMAINING_SERVICES --timeout=60s || true
          else
            echo "â„¹ï¸ No remaining services found"
          fi

      - name: Force Cleanup Stuck Resources
        if: github.event.inputs.cleanup_type == 'all-k8s'
        run: |
          echo "ðŸ”¥ Force cleaning any stuck resources..."

          # Force delete any remaining pods
          kubectl delete pods -l app=${{ env.APP_NAME }} --force --grace-period=0 || true

          # Remove any finalizers that might be blocking deletion
          kubectl patch deployment ${{ env.APP_NAME }} -p '{"metadata":{"finalizers":[]}}' --type=merge || true

      - name: Post-Cleanup Status
        run: |
          echo "ðŸ“Š Post-Cleanup Kubernetes Status:"
          echo "==================================="

          echo "ðŸ” Helm Releases:"
          helm list -A | grep ${{ env.APP_NAME }} || echo "âœ… No Helm releases found"

          echo ""
          echo "ðŸ” Deployments:"
          kubectl get deployments -l app=${{ env.APP_NAME }} -o wide || echo "âœ… No deployments found"

          echo ""
          echo "ðŸ” Pods:"
          kubectl get pods -l app=${{ env.APP_NAME }} -o wide || echo "âœ… No pods found"

          echo ""
          echo "ðŸ” Services:"
          kubectl get services -l app=${{ env.APP_NAME }} -o wide || echo "âœ… No services found"

          echo ""
          echo "ðŸ” AWS LoadBalancer Status:"
          aws elbv2 describe-load-balancers --query "LoadBalancers[?contains(LoadBalancerName, '${{ env.APP_NAME }}') || contains(LoadBalancerName, 'a4a3c58a')].{Name:LoadBalancerName,State:State.Code}" --output table || echo "âœ… No LoadBalancers found"

      - name: Final LoadBalancer Cleanup Verification
        run: |
          echo "ðŸ” Final verification - checking for any remaining LoadBalancers..."

          # Wait a bit more for AWS propagation
          sleep 30

          # Check for any LoadBalancers that might still be deleting
          REMAINING_LBS=$(aws elbv2 describe-load-balancers --query "LoadBalancers[?contains(LoadBalancerName, '${{ env.APP_NAME }}') || contains(LoadBalancerName, 'a4a3c58a')].LoadBalancerName" --output text 2>/dev/null || echo "")

          if [ ! -z "$REMAINING_LBS" ] && [ "$REMAINING_LBS" != "None" ]; then
            echo "âš ï¸ Warning: Some LoadBalancers may still be in deletion process:"
            echo "$REMAINING_LBS"
            echo "â„¹ï¸ This is normal - AWS LoadBalancer deletion can take 2-5 minutes"
            echo "â„¹ï¸ Verify manually if infrastructure destruction fails"
          else
            echo "âœ… All LoadBalancers successfully cleaned up"
          fi

      - name: Cleanup Summary
        if: always()
        run: |
          echo "### ðŸ§¹ Helm Cleanup Summary" >> $GITHUB_STEP_SUMMARY
          echo "| Resource | Action | Status |" >> $GITHUB_STEP_SUMMARY
          echo "| -------- | ------ | ------ |" >> $GITHUB_STEP_SUMMARY
          echo "| Cleanup Type | ${{ github.event.inputs.cleanup_type }} | âœ… |" >> $GITHUB_STEP_SUMMARY
          echo "| Environment | ${{ env.ENVIRONMENT }} | âœ… |" >> $GITHUB_STEP_SUMMARY
          echo "| Deployment Method | Helm | Security Enhanced |" >> $GITHUB_STEP_SUMMARY
          echo "| Cluster | ${{ env.CLUSTER_NAME }} | âœ… |" >> $GITHUB_STEP_SUMMARY
          echo "| Region | ${{ env.AWS_REGION }} | âœ… |" >> $GITHUB_STEP_SUMMARY
          echo "| Helm Releases | Uninstalled | All ${{ env.APP_NAME }} releases |" >> $GITHUB_STEP_SUMMARY
          echo "| LoadBalancer Cleanup | âœ… | AWS ELB/NLB deleted |" >> $GITHUB_STEP_SUMMARY
          echo "| Status | ${{ job.status }} | Job Status |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Helm cleanup completed!** All releases and LoadBalancers have been properly cleaned up." >> $GITHUB_STEP_SUMMARY
          echo "You can now safely run infrastructure destruction without dependency issues." >> $GITHUB_STEP_SUMMARY
